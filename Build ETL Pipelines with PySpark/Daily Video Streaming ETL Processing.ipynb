{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2def67b-7990-4665-a806-512d3f008120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Daily Video Streaming ETL Processing\n",
    "\n",
    "This notebook processes daily video streaming events data to calculate key metrics.\n",
    "\n",
    "**Parameters:**\n",
    "- **execution_date**: The date to process in YYYY-MM-DD format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4c4f15f-3263-4199-8b45-67bb38cbd95e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up widgets for parameters\n",
    "dbutils.widgets.text(\"execution_date\", \"\", \"Execution Date (YYYY-MM-DD)\")\n",
    "\n",
    "# Get the execution date parameter\n",
    "execution_date = dbutils.widgets.get(\"execution_date\")\n",
    "print(f\"Raw execution_date parameter received: '{execution_date}'\")\n",
    "\n",
    "# Handle potential date format issues\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# Clean up the execution date if it has extra characters\n",
    "def clean_date_string(date_str):\n",
    "    \"\"\"Extract YYYY-MM-DD format from various possible date formats\"\"\"\n",
    "    # If empty, use current date\n",
    "    if not date_str or date_str.strip() == \"\":\n",
    "        return datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "    # Try to find a pattern that looks like YYYY-MM-DD\n",
    "    match = re.search(r'(\\d{4})-(\\d{1,2})-(\\d{1,2})', date_str)\n",
    "    if match:\n",
    "        # Extract the matched date parts\n",
    "        year, month, day = match.groups()\n",
    "        # Pad month and day with leading zeros if needed\n",
    "        month = month.zfill(2)\n",
    "        day = day.zfill(2)\n",
    "        return f\"{year}-{month}-{day}\"\n",
    "    \n",
    "    # Try other common formats\n",
    "    try:\n",
    "        # Try to parse as timestamp if it's a long number\n",
    "        if date_str.isdigit() and len(date_str) > 8:\n",
    "            timestamp = int(date_str) / 1000 if len(date_str) > 10 else int(date_str)\n",
    "            dt = datetime.datetime.fromtimestamp(timestamp)\n",
    "            return dt.strftime(\"%Y-%m-%d\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Return current date as fallback\n",
    "    print(f\"Could not parse date format from '{date_str}', using current date\")\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Clean up the date string\n",
    "clean_execution_date = clean_date_string(execution_date)\n",
    "if clean_execution_date != execution_date:\n",
    "    print(f\"Cleaned execution date to: '{clean_execution_date}'\")\n",
    "    execution_date = clean_execution_date\n",
    "\n",
    "# Final validation\n",
    "try:\n",
    "    parsed_date = datetime.datetime.strptime(execution_date, \"%Y-%m-%d\")\n",
    "    print(f\"Using execution date: {parsed_date.strftime('%Y-%m-%d')}\")\n",
    "except ValueError:\n",
    "    # If we still have an invalid date after cleaning, use current date\n",
    "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    print(f\"Invalid date format after cleaning. Using current date: {current_date}\")\n",
    "    execution_date = current_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1fb844f-52ba-48c6-a5f0-fde9ab1dad19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0658c139-e5f5-48b1-916f-b2e2fad13b39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "base_path = \"/pyspark/video-streaming-data\"\n",
    "events_path = f\"{base_path}/module5-orchestration/scheduling/daily_events\"\n",
    "file_path = f\"{events_path}/events_{execution_date}.csv\"\n",
    "output_path = f\"{base_path}/module5-orchestration/scheduling/processed_metrics/date={execution_date}\"\n",
    "\n",
    "print(f\"Input path: {file_path}\")\n",
    "print(f\"Output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edd37d5a-4b89-4627-9250-0df2b9981c5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extract: Read Daily Events Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddc8a43c-b92d-4f96-8804-22f4a9bc51a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if file exists\n",
    "import os\n",
    "\n",
    "# Function to check for alternative dates if file is not found\n",
    "def find_alternative_date(date_str, base_path, num_days=5):\n",
    "    \"\"\"Find a nearby date that has data if the requested date doesn't have data\"\"\"\n",
    "    try:\n",
    "        original_date = datetime.datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        \n",
    "        # Try up to num_days before and after the requested date\n",
    "        for i in range(1, num_days + 1):\n",
    "            # Try earlier dates\n",
    "            earlier_date = original_date - datetime.timedelta(days=i)\n",
    "            earlier_str = earlier_date.strftime(\"%Y-%m-%d\")\n",
    "            earlier_path = f\"{base_path}/events_{earlier_str}.csv\"\n",
    "            if os.path.exists(f\"/dbfs/{earlier_path}\"):\n",
    "                return earlier_str\n",
    "                \n",
    "            # Try later dates\n",
    "            later_date = original_date + datetime.timedelta(days=i)\n",
    "            later_str = later_date.strftime(\"%Y-%m-%d\")\n",
    "            later_path = f\"{base_path}/events_{later_str}.csv\"\n",
    "            if os.path.exists(f\"/dbfs/{later_path}\"):\n",
    "                return later_str\n",
    "                \n",
    "        return None  # No alternative found\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(f\"/dbfs/{file_path}\"):\n",
    "    print(f\"WARNING: No data file found for date {execution_date}\")\n",
    "    \n",
    "    # Try to find an alternative date\n",
    "    alt_date = find_alternative_date(execution_date, events_path)\n",
    "    if alt_date:\n",
    "        print(f\"Found alternative date with data: {alt_date}\")\n",
    "        execution_date = alt_date\n",
    "        file_path = f\"{events_path}/events_{execution_date}.csv\"\n",
    "        output_path = f\"{base_path}/module5-orchestration/scheduling/processed_metrics/date={execution_date}\"\n",
    "        print(f\"Using file: {file_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No data file found for date {execution_date} or nearby dates\")\n",
    "\n",
    "# Read the daily events\n",
    "print(f\"Reading events from: {file_path}\")\n",
    "daily_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(file_path)\n",
    "\n",
    "# Print record count and schema\n",
    "print(f\"Record count: {daily_df.count()}\")\n",
    "print(\"Schema:\")\n",
    "daily_df.printSchema()\n",
    "\n",
    "# Show sample data\n",
    "display(daily_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16e33fa4-c8c6-4c5a-8638-9007900976a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Data cleaning\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "# Remove duplicates and handle nulls\n",
    "cleaned_df = daily_df \\\n",
    "    .dropDuplicates([\"event_id\"]) \\\n",
    "    .na.fill(\"\", [\"error_type\"]) \\\n",
    "    .na.fill(0, [\"buffering_count\"]) \\\n",
    "    .filter(col(\"duration_seconds\").isNotNull())\n",
    "\n",
    "print(f\"Record count after cleaning: {cleaned_df.count()}\")\n",
    "\n",
    "# Add processing timestamp\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "cleaned_df = cleaned_df.withColumn(\"processing_time\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c065548-fdda-4525-bbd4-220d665a3c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Calculate basic metrics\n",
    "from pyspark.sql.functions import count, avg, sum, round as spark_round, max as spark_max\n",
    "\n",
    "# Calculate metrics by device type and country\n",
    "device_country_metrics = cleaned_df.groupBy(\"device_type\", \"country\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        spark_round(avg(\"duration_seconds\"), 2).alias(\"avg_duration_seconds\"),\n",
    "        sum(when(col(\"error_type\") != \"\", 1).otherwise(0)).alias(\"error_count\"),\n",
    "        spark_max(\"duration_seconds\").alias(\"max_duration_seconds\"),\n",
    "        spark_round(avg(\"buffering_count\"), 2).alias(\"avg_buffering_count\")\n",
    "    )\n",
    "\n",
    "print(\"Device and Country Metrics:\")\n",
    "display(device_country_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04264fd8-ef14-412d-93fa-4637442ab8d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Calculate content popularity metrics\n",
    "content_metrics = cleaned_df.groupBy(\"content_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"view_count\"),\n",
    "        spark_round(avg(\"duration_seconds\"), 2).alias(\"avg_view_duration\"),\n",
    "        sum(when(col(\"error_type\") != \"\", 1).otherwise(0)).alias(\"error_count\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"view_count\").desc())\n",
    "\n",
    "print(\"Top 10 Content by Views:\")\n",
    "display(content_metrics.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cea0a15d-e183-4c27-9e9b-efedafa6f4f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Calculate hourly distribution\n",
    "from pyspark.sql.functions import hour, from_utc_timestamp, col\n",
    "\n",
    "# Extract hour from timestamp\n",
    "hourly_df = cleaned_df.withColumn(\n",
    "    \"hour\", hour(from_utc_timestamp(col(\"timestamp\"), \"UTC\"))\n",
    ")\n",
    "\n",
    "# Calculate metrics by hour\n",
    "hourly_metrics = hourly_df.groupBy(\"hour\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"event_count\"),\n",
    "        spark_round(avg(\"duration_seconds\"), 2).alias(\"avg_duration\")\n",
    "    ) \\\n",
    "    .orderBy(\"hour\")\n",
    "\n",
    "print(\"Hourly Distribution:\")\n",
    "display(hourly_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "118bb193-4d1b-4e05-94ab-fc643e16326e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load: Save Processed Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bd67a1b-034c-4b24-b3f5-b4d02cb83f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "dbutils.fs.mkdirs(f\"{output_path}\")\n",
    "\n",
    "# Save the primary metrics\n",
    "print(f\"Saving device/country metrics to: {output_path}/device_country_metrics\")\n",
    "device_country_metrics.write.mode(\"overwrite\").parquet(f\"{output_path}/device_country_metrics\")\n",
    "\n",
    "print(f\"Saving content metrics to: {output_path}/content_metrics\")\n",
    "content_metrics.write.mode(\"overwrite\").parquet(f\"{output_path}/content_metrics\")\n",
    "\n",
    "print(f\"Saving hourly metrics to: {output_path}/hourly_metrics\")\n",
    "hourly_metrics.write.mode(\"overwrite\").parquet(f\"{output_path}/hourly_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca60c4b9-a824-4df2-8ec9-e3dc7a2f6203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0225e05-8211-4641-849f-2a3a59595adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform data quality checks\n",
    "from pyspark.sql.functions import count, when, col, isnan\n",
    "\n",
    "# Count total records processed\n",
    "total_records = daily_df.count()\n",
    "records_after_cleaning = cleaned_df.count()\n",
    "dropped_record_count = total_records - records_after_cleaning\n",
    "\n",
    "# Count records written\n",
    "device_country_count = device_country_metrics.count()\n",
    "content_metrics_count = content_metrics.count()\n",
    "hourly_metrics_count = hourly_metrics.count()\n",
    "\n",
    "# Check for negative durations\n",
    "negative_durations = cleaned_df.filter(col(\"duration_seconds\") < 0).count()\n",
    "\n",
    "# Create a quality check summary\n",
    "quality_checks = spark.createDataFrame([\n",
    "    (\"total_input_records\", total_records),\n",
    "    (\"records_after_cleaning\", records_after_cleaning),\n",
    "    (\"dropped_record_count\", dropped_record_count),\n",
    "    (\"device_country_metrics_count\", device_country_count),\n",
    "    (\"content_metrics_count\", content_metrics_count),\n",
    "    (\"hourly_metrics_count\", hourly_metrics_count),\n",
    "    (\"records_with_negative_duration\", negative_durations)\n",
    "], [\"check_name\", \"value\"])\n",
    "\n",
    "# Save quality checks\n",
    "quality_checks.write.mode(\"overwrite\").parquet(f\"{output_path}/quality_checks\")\n",
    "\n",
    "# Display quality summary\n",
    "print(\"Data Quality Check Summary:\")\n",
    "display(quality_checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00d59d0a-2ca2-4c68-b021-75890953d5a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary and Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8dc068d-8718-4f2b-868b-d05a63ead5df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"ETL processing completed successfully for date: {execution_date}\")\n",
    "print(f\"Processed {records_after_cleaning} records after cleaning\")\n",
    "print(f\"Generated {device_country_count} device/country metric records\")\n",
    "print(f\"Generated {content_metrics_count} content metric records\")\n",
    "print(f\"Generated {hourly_metrics_count} hourly metric records\")\n",
    "print(f\"All metrics saved to: {output_path}\")\n",
    "\n",
    "# Return success for the scheduler\n",
    "dbutils.notebook.exit({\n",
    "    \"status\": \"success\",\n",
    "    \"date\": execution_date,\n",
    "    \"record_count\": records_after_cleaning,\n",
    "    \"processing_time\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Daily Video Streaming ETL Processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}