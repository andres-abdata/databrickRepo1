{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36217e2b-1cb1-4c28-bf95-cde5ad7bc011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Real-time ETL & Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0439fa87-dc56-41ee-adb1-c1f039632f17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries for Structured Streaming\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff5319f7-fe24-4788-83d7-e418e8d2f782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's define our schema for the streaming data\n",
    "# This is important for streaming to ensure consistent schema interpretation\n",
    "schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"content_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"duration_seconds\", IntegerType(), True),\n",
    "    StructField(\"device_type\", StringType(), True),\n",
    "    StructField(\"quality\", StringType(), True),\n",
    "    StructField(\"buffering_count\", IntegerType(), True),\n",
    "    StructField(\"error_type\", StringType(), True),\n",
    "    StructField(\"ip_address\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"session_id\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79591fce-e50e-40fe-bb7a-722257fe4b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now, let's create a streaming DataFrame that reads from our stream source\n",
    "# This could be a directory where new files arrive, Kafka, or other streaming sources\n",
    "streaming_df = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .json(\"/pyspark/video-streaming-data/module5-orchestration/streaming/stream_source\")\n",
    "\n",
    "print(\"Streaming source initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a43518a-4f01-41c4-a310-fbffca2c816f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's do some basic transformations on our streaming data\n",
    "# We'll parse the timestamp, calculate metrics, and prepare data for analysis\n",
    "\n",
    "# Parse timestamp into proper timestamp type and extract date components\n",
    "parsed_df = streaming_df \\\n",
    "    .withColumn(\"event_timestamp\", to_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\")) \\\n",
    "    .withColumn(\"event_date\", to_date(\"event_timestamp\")) \\\n",
    "    .withColumn(\"event_hour\", hour(\"event_timestamp\")) \\\n",
    "    .withColumn(\"event_minute\", minute(\"event_timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9692e8ce-0a36-4d4b-9a78-71e7aa990919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now, let's perform some streaming aggregations\n",
    "# Calculate metrics in real-time with a 1-minute window\n",
    "\n",
    "# Define a 1-minute tumbling window\n",
    "windowed_counts = parsed_df \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(\"event_timestamp\", \"1 minute\"),\n",
    "        \"device_type\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"event_count\"),\n",
    "        avg(\"duration_seconds\").alias(\"avg_duration\"),\n",
    "        sum(\"buffering_count\").alias(\"total_buffering_events\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6654594-4dfa-4af4-9442-e34987ab3dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's also track errors in our streaming data\n",
    "error_tracking = parsed_df \\\n",
    "    .filter(col(\"error_type\").isNotNull()) \\\n",
    "    .withWatermark(\"event_timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(\"event_timestamp\", \"1 minute\"),\n",
    "        \"error_type\"\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"error_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d532d59-5c67-4d9b-9913-69370f22fd73",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "streaming query"
    }
   },
   "outputs": [],
   "source": [
    "# Now we'll start our streaming query to process the data\n",
    "# This query will continuously process new data as it arrives\n",
    "\n",
    "# Start the query to process streaming metrics\n",
    "# Note the use of a trigger to control processing frequency\n",
    "query = windowed_counts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"streaming_device_metrics\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "# Start the error tracking query with metrics enabled\n",
    "# This allows us to monitor the performance of this specific query\n",
    "error_query = error_tracking \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"streaming_errors\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming queries started!\")\n",
    "print(f\"Active streaming queries: {spark.streams.active}\")\n",
    "print(f\"Total active streaming queries: {len(spark.streams.active)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57cac1fd-96cf-4ae7-9182-b1867ccbe156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now let's set up our monitoring - we'll check the query status\n",
    "print(\"Streaming Query Status:\")\n",
    "print(f\"Query name: {query.name}\")\n",
    "print(f\"Query status: {query.status}\")\n",
    "print(f\"Is active: {query.isActive}\")\n",
    "print(f\"Recent progress:\")\n",
    "for progress in query.recentProgress[-5:]:\n",
    "    print(f\"  Batch: {progress['batchId']}, # rows: {progress.get('numInputRows', 'N/A')}, \" + \n",
    "          f\"Processing time: {progress.get('batchDuration', 'N/A')}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb2979f-0eb8-4a2c-bbf9-0afcc6316e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's examine our streaming metrics more closely\n",
    "# We can access detailed metrics about our streaming query\n",
    "\n",
    "# Get the latest metrics from our streaming query\n",
    "latest_metrics = query.lastProgress\n",
    "\n",
    "if latest_metrics:\n",
    "    print(\"Latest Streaming Metrics:\")\n",
    "    print(f\"Input rate: {latest_metrics.get('inputRowsPerSecond', 'N/A')} rows/second\")\n",
    "    print(f\"Processing rate: {latest_metrics.get('processedRowsPerSecond', 'N/A')} rows/second\")\n",
    "    print(f\"Batch duration: {latest_metrics.get('batchDuration', 'N/A')} ms\")\n",
    "    print(f\"Operation duration: {latest_metrics.get('totalDuration', 'N/A')} ms\")\n",
    "    \n",
    "    # Advanced metrics for debugging performance issues\n",
    "    stateOperators = latest_metrics.get('stateOperators', [])\n",
    "    if stateOperators:\n",
    "        print(\"\\nState Operation Metrics:\")\n",
    "        for operator in stateOperators:\n",
    "            print(f\"  - {operator.get('operatorName')}: {operator.get('numRowsTotal')} total rows in state\")\n",
    "            print(f\"    Memory used: {operator.get('memoryUsedBytes')/1024/1024:.2f} MB\")\n",
    "else:\n",
    "    print(\"No metrics available yet - query just started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ed916f6-88ea-4548-b9fd-c716232fdb89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from streaming_device_metrics\").show(truncate=False)\n",
    "spark.sql(\"select count(*) from streaming_errors\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a684ac-d268-4e76-80fa-efb4b4b7c244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can query the in-memory tables to see results in real-time\n",
    "# In a production environment, you'd likely write to a more permanent storage\n",
    "print(\"Current device metrics:\")\n",
    "spark.sql(\"SELECT * FROM streaming_device_metrics ORDER BY window DESC LIMIT 10\").display(truncate=False)\n",
    "\n",
    "print(\"Current error metrics:\")\n",
    "spark.sql(\"SELECT * FROM streaming_errors ORDER BY window DESC, error_count DESC LIMIT 10\").display(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96f5e9ec-c6b8-46e1-9fe0-7f38b2e230c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's demonstrate writing our streaming results to Delta Lake\n",
    "# This is a common pattern for building real-time data lakehouse architectures\n",
    "\n",
    "# Using complete output mode for aggregations - this overwrites the entire result table each time\n",
    "delta_query = parsed_df \\\n",
    "    .groupBy(\n",
    "        \"event_date\",\n",
    "        \"event_hour\",\n",
    "        \"device_type\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"event_count\"),\n",
    "        avg(\"duration_seconds\").alias(\"avg_duration\")\n",
    "    ) \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", \"/pyspark/video-streaming-data/module5-orchestration/streaming/delta_checkpoints\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start(\"/pyspark/video-streaming-data/module5-orchestration/streaming/delta_metrics\")\n",
    "\n",
    "print(\"Started writing streaming aggregations to Delta Lake!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54eef0bb-6117-432a-a140-8b1064b223c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Generator for Real-time Demo\n",
    "Let's create and inject some sample data to demonstrate real-time data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "082d8411-e95d-4973-98db-dcd4032a5737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a function to generate sample streaming events\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "def generate_streaming_events(num_events=100, delay_seconds=0.2):\n",
    "    \"\"\"Generate sample streaming events and write them to the source directory\"\"\"\n",
    "    \n",
    "    # Define data generation parameters\n",
    "    device_types = [\"TV\", \"Mobile\", \"Web\", \"Tablet\"]\n",
    "    qualities = [\"SD\", \"HD\", \"4K\"]\n",
    "    error_types = [None, None, None, None, \"network_error\", \"server_error\", \"content_unavailable\"]\n",
    "    countries = [\"US\", \"CA\", \"UK\", \"FR\", \"DE\", \"JP\", \"BR\", \"AU\"]\n",
    "    \n",
    "    # Source directory for streaming data\n",
    "    stream_dir = \"/dbfs/pyspark/video-streaming-data/module5-orchestration/streaming/stream_source\"\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(stream_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Generating {num_events} streaming events...\")\n",
    "    \n",
    "    # Generate events in real-time\n",
    "    for i in range(num_events):\n",
    "        event_id = f\"EVT{uuid.uuid4().hex[:8]}\"\n",
    "        user_id = f\"USR{random.randint(10000, 99999)}\"\n",
    "        content_id = f\"CON{random.randint(10000, 99999)}\"\n",
    "        \n",
    "        # Use current time for timestamp to see real-time effects\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        \n",
    "        # Generate a random event\n",
    "        event = {\n",
    "            \"event_id\": event_id,\n",
    "            \"user_id\": user_id,\n",
    "            \"content_id\": content_id,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"duration_seconds\": random.randint(10, 3600),\n",
    "            \"device_type\": random.choice(device_types),\n",
    "            \"quality\": random.choice(qualities),\n",
    "            \"buffering_count\": random.randint(0, 10),\n",
    "            \"error_type\": random.choice(error_types),\n",
    "            \"ip_address\": f\"192.168.{random.randint(1, 255)}.{random.randint(1, 255)}\",\n",
    "            \"country\": random.choice(countries),\n",
    "            \"session_id\": f\"SES{uuid.uuid4().hex[:8]}\"\n",
    "        }\n",
    "        \n",
    "        # Write the event to a JSON file\n",
    "        filename = f\"{stream_dir}/{event_id}.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(json.dumps(event))\n",
    "        \n",
    "        # Print progress\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Generated {i + 1} events...\")\n",
    "        \n",
    "        # Add a small delay to simulate streaming\n",
    "        time.sleep(delay_seconds)\n",
    "    \n",
    "    print(f\"Successfully generated {num_events} streaming events\")\n",
    "\n",
    "# Execute the function to start generating data\n",
    "# Adjust the number of events and delay as needed\n",
    "generate_streaming_events(num_events=50, delay_seconds=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "434a8adb-f47a-4a2e-a2d8-150e7adbba61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now let's look at integrating system metrics for holistic monitoring\n",
    "# We'll load a sample of metrics that would typically come from a monitoring system\n",
    "metrics_df = spark.read.csv(\"/pyspark/video-streaming-data/module5-orchestration/streaming/monitoring/job_metrics.csv\", \n",
    "                           header=True, inferSchema=True)\n",
    "\n",
    "print(\"Sample system metrics for monitoring:\")\n",
    "metrics_df.select(\"timestamp\", \"job_id\", \"executor_cores\", \"executor_memory\", \"duration_ms\", \"status\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf8f512-2508-45e3-93f6-fc8933e66fdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a dashboard for operational monitoring would typically involve:\n",
    "# 1. Collecting metrics from Spark's metrics system\n",
    "# 2. Storing them in a time-series database (like InfluxDB, Prometheus)\n",
    "# 3. Visualizing with tools like Grafana\n",
    "\n",
    "# Here's how you could export metrics for external dashboarding\n",
    "metrics_for_export = metrics_df \\\n",
    "    .withColumn(\"timestamp\", to_timestamp(\"timestamp\")) \\\n",
    "    .withColumn(\"is_long_running\", when(col(\"duration_ms\") > 10000, 1).otherwise(0))\n",
    "\n",
    "# Write metrics for dashboard consumption\n",
    "metrics_for_export.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"/pyspark/video-streaming-data/module5-orchestration/streaming/monitoring/dashboard_metrics\")\n",
    "\n",
    "print(\"Metrics prepared for dashboard integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b22510-d33b-4a7d-84f6-21a4983ee53b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up alerts based on streaming metrics\n",
    "# In a real environment, this could trigger emails, Slack notifications, etc.\n",
    "\n",
    "# Simple alert logic example\n",
    "alert_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  window.end as alert_time,\n",
    "  error_type,\n",
    "  error_count\n",
    "FROM streaming_errors\n",
    "WHERE error_count > 5\n",
    "ORDER BY window.end DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Recent alerts that would trigger notifications:\")\n",
    "alert_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "417230f8-8708-4455-a588-716ef19d61c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To properly shut down our streaming queries when done\n",
    "# This is important for clean resource management\n",
    "print(\"Stopping streaming queries...\")\n",
    "query.stop()\n",
    "error_query.stop()\n",
    "delta_query.stop()\n",
    "print(\"All streaming queries stopped!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5644465061461282,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Module 5, Clip 14 - Real-time ETL & Monitoring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}