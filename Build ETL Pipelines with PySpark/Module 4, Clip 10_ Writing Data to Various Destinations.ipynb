{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f844a3b6-2b66-45f5-b526-2bac86be384e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Writing Data to Various Destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22448065-0e3d-4448-adc5-30f89192cdf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of our streaming data:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>event_id</th><th>user_id</th><th>content_id</th><th>timestamp</th><th>duration_seconds</th><th>device_type</th><th>quality</th><th>buffering_count</th><th>error_type</th><th>ip_address</th><th>country</th><th>session_id</th></tr></thead><tbody><tr><td>EVT10000</td><td>USR41813</td><td>CON10763</td><td>2023-09-03T09:18:59Z</td><td>565</td><td>Web</td><td>HD</td><td>4</td><td>null</td><td>72.119.240.124</td><td>ES</td><td>SES10000</td></tr><tr><td>EVT10001</td><td>USR46484</td><td>CON12784</td><td>2023-09-09T11:44:27Z</td><td>2018</td><td>Web</td><td>HD</td><td>1</td><td>null</td><td>156.3.251.123</td><td>FR</td><td>SES10001</td></tr><tr><td>EVT10002</td><td>USR37573</td><td>CON16367</td><td>2023-09-09T16:51:53Z</td><td>2900</td><td>TV</td><td>4K</td><td>3</td><td>null</td><td>182.53.26.241</td><td>AU</td><td>SES10002</td></tr><tr><td>EVT10003</td><td>USR46584</td><td>CON18916</td><td>2023-09-13T08:03:13Z</td><td>3242</td><td>Tablet</td><td>4K</td><td>3</td><td>null</td><td>9.203.70.180</td><td>FR</td><td>SES10003</td></tr><tr><td>EVT10004</td><td>USR52241</td><td>CON18924</td><td>2023-09-04T13:07:20Z</td><td>4248</td><td>TV</td><td>4K</td><td>1</td><td>null</td><td>152.202.251.124</td><td>NL</td><td>SES10004</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "EVT10000",
         "USR41813",
         "CON10763",
         "2023-09-03T09:18:59Z",
         565,
         "Web",
         "HD",
         4,
         null,
         "72.119.240.124",
         "ES",
         "SES10000"
        ],
        [
         "EVT10001",
         "USR46484",
         "CON12784",
         "2023-09-09T11:44:27Z",
         2018,
         "Web",
         "HD",
         1,
         null,
         "156.3.251.123",
         "FR",
         "SES10001"
        ],
        [
         "EVT10002",
         "USR37573",
         "CON16367",
         "2023-09-09T16:51:53Z",
         2900,
         "TV",
         "4K",
         3,
         null,
         "182.53.26.241",
         "AU",
         "SES10002"
        ],
        [
         "EVT10003",
         "USR46584",
         "CON18916",
         "2023-09-13T08:03:13Z",
         3242,
         "Tablet",
         "4K",
         3,
         null,
         "9.203.70.180",
         "FR",
         "SES10003"
        ],
        [
         "EVT10004",
         "USR52241",
         "CON18924",
         "2023-09-04T13:07:20Z",
         4248,
         "TV",
         "4K",
         1,
         null,
         "152.202.251.124",
         "NL",
         "SES10004"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "event_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "content_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "duration_seconds",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "device_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quality",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "buffering_count",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "error_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ip_address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "session_id",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, let's load our processed streaming data\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load our sample streaming data that we processed in previous modules\n",
    "file_path = \"/pyspark/video-streaming-data/module3-transform/joins_aggregations/streaming_events.csv\"\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "# Show our data\n",
    "print(\"Sample of our streaming data:\")\n",
    "df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce60137f-778d-4c82-9977-8b0f50103a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 50000\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many records we have\n",
    "print(f\"Total records: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b6f3757-8356-4764-9a02-9b17e651efb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. WRITE TO PARQUET FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26bb25e2-1102-4b40-82d0-6593c58d1986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parquet is a columnar format that compresses data and provides efficient queries\n",
    "parquet_path = \"/pyspark/video-streaming-data/module4-load/destinations/output_templates/parquet_output\"\n",
    "\n",
    "# Write the DataFrame to Parquet\n",
    "df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "\n",
    "print(f\"Data written to Parquet at: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca80616-7e39-4755-9ca7-6e72adeeceee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Read it back to verify (this is faster than CSV because of Parquet optimizations)\n",
    "parquet_df = spark.read.parquet(parquet_path)\n",
    "print(\"Data read back from Parquet:\")\n",
    "parquet_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ee26e7d-d507-4816-a57a-e96b0047d71e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2. WRITE TO ORC FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40f63797-1d96-4333-8741-03f8c970c078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ORC is another columnar format with good compression and performance\n",
    "orc_path = \"/pyspark/video-streaming-data/module4-load/destinations/output_templates/orc_output\"\n",
    "\n",
    "# Write the DataFrame to ORC\n",
    "df.write.mode(\"overwrite\").orc(orc_path)\n",
    "\n",
    "print(f\"Data written to ORC at: {orc_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c57f8b9-e923-4fdf-83e3-43e3060059b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3. WRITE TO DELTA LAKE FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36769846-286b-48f8-b6fd-7680667c94d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delta Lake adds ACID transactions, versioning, and many other features\n",
    "delta_path = \"/pyspark/video-streaming-data/module4-load/destinations/output_templates/delta_output\"\n",
    "\n",
    "# Write the DataFrame to Delta\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "print(f\"Data written to Delta Lake at: {delta_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5514bea0-bcd2-4b6c-9fad-03c45b6e0dee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read back Delta Lake to verify\n",
    "delta_df = spark.read.format(\"delta\").load(delta_path)\n",
    "print(\"First 5 rows from Delta Lake:\")\n",
    "delta_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "011a07cd-c8fc-4d9c-b306-20283352af90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4. PARTITIONING DATA FOR OPTIMIZED QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e4a57ee-242b-4baa-bc5b-cec0e31d628f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's partition the data by device_type to optimize for queries that filter on this column\n",
    "partitioned_path = \"/pyspark/video-streaming-data/module4-load/destinations/output_templates/partitioned\"\n",
    "\n",
    "# Write with partitioning\n",
    "df.write \\\n",
    "    .partitionBy(\"device_type\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(partitioned_path)\n",
    "\n",
    "print(f\"Data partitioned by device_type and written to: {partitioned_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a60166d9-23c7-4615-a985-c90f5f447e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List the partitions that were created\n",
    "# In standard Jupyter, you would use os.listdir instead of dbutils.fs.ls\n",
    "import os\n",
    "partitions = os.listdir(partitioned_path)\n",
    "for partition in partitions:\n",
    "    print(partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "375c5ce2-6e8f-45cc-a6cb-c06445a2dc91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 5. WRITING TO DATABASE TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c1232ea-9435-464a-aa2c-d3ad0ed01bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create widgets for database connection parameters\n",
    "dbutils.widgets.text(\"jdbc_server\", \"your-azure-sql-server.database.windows.net\", \"JDBC Server\")\n",
    "dbutils.widgets.text(\"jdbc_database\", \"videostreamingdb\", \"Database Name\")\n",
    "dbutils.widgets.text(\"jdbc_table\", \"streamingevents\", \"Table Name\")\n",
    "dbutils.widgets.text(\"jdbc_user\", \"admin\", \"Username\")\n",
    "dbutils.widgets.text(\"jdbc_password\", \"your_password_here\", \"Password\") # Using text widget for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4245aba3-f5e6-4549-87d1-04f69f72fd4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get values from widgets\n",
    "jdbc_server = dbutils.widgets.get(\"jdbc_server\")\n",
    "jdbc_database = dbutils.widgets.get(\"jdbc_database\")\n",
    "jdbc_table = dbutils.widgets.get(\"jdbc_table\")\n",
    "jdbc_user = dbutils.widgets.get(\"jdbc_user\")\n",
    "jdbc_password = dbutils.widgets.get(\"jdbc_password\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f280a89d-e58b-4f68-ae86-e4a129e15bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed widget: jdbc_database\nRemoved widget: jdbc_password\nRemoved widget: jdbc_server\nRemoved widget: jdbc_user\nRemoved widget: jdbc_table\n"
     ]
    }
   ],
   "source": [
    "# Remove SQL widgets to clean up the UI\n",
    "widget_names = [\"jdbc_database\", \"jdbc_password\", \"jdbc_server\", \"jdbc_user\", \"jdbc_table\"]\n",
    "\n",
    "# Try to remove all widgets\n",
    "for name in widget_names:\n",
    "    try:\n",
    "        dbutils.widgets.remove(name)\n",
    "        print(f\"Removed widget: {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to remove widget {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a268c54-45ed-4347-80bb-0dd09510a353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured to write to: 57.database.windows.net/videostreamingdb/streamingevents\n"
     ]
    }
   ],
   "source": [
    "# Construct JDBC URL\n",
    "jdbc_url = f\"jdbc:sqlserver://{jdbc_server}:1433;database={jdbc_database}\"\n",
    "\n",
    "# Define driver\n",
    "jdbc_driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "\n",
    "print(f\"Configured to write to: {jdbc_server}/{jdbc_database}/{jdbc_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de22bbee-5cc7-4bd6-97d2-61ea4482fdac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to database table with optimizations for our dataset\n"
     ]
    }
   ],
   "source": [
    "# For a small dataset (50,000 rows), fewer partitions are better\n",
    "# Too many partitions would create unnecessary overhead\n",
    "optimized_df = df.repartition(4)  # 4 partitions for 50,000 rows\n",
    "\n",
    "# Write to a database table with optimizations appropriate for this size\n",
    "optimized_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", jdbc_table) \\\n",
    "    .option(\"user\", jdbc_user) \\\n",
    "    .option(\"password\", jdbc_password) \\\n",
    "    .option(\"driver\", jdbc_driver) \\\n",
    "    .option(\"batchsize\", 5000) \\\n",
    "    .option(\"isolationLevel\", \"READ_COMMITTED\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data written to database table with optimizations for our dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b61f4d1-7a71-47b5-b752-0ecb11fba970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6. Z-ORDERING FOR PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96028e4d-104c-4a3a-a61c-d1364e158132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading large dataset...\nDataset loaded: 100,000 rows\n"
     ]
    }
   ],
   "source": [
    "# 1. Set up paths\n",
    "base_dir = \"/pyspark/video-streaming-data\"\n",
    "large_data_path = f\"{base_dir}/module3-transform/optimization/large_events.csv\"\n",
    "zordered_path = f\"{base_dir}/module4-load/destinations/output_templates/z_ordered/events\"\n",
    "unoptimized_path = f\"{base_dir}/module4-load/destinations/output_templates/unoptimized/events\"\n",
    "\n",
    "# 2. Load and prepare data\n",
    "print(\"Loading large dataset...\")\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(large_data_path)\n",
    "print(f\"Dataset loaded: {df.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d26621d-ccbe-4c70-b01b-78a46117c7ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating unoptimized Delta table...\nCreating and Z-ordering Delta table...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Create unoptimized Delta table\n",
    "print(\"Creating unoptimized Delta table...\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(unoptimized_path)\n",
    "\n",
    "# 4. Create and Z-order Delta table\n",
    "print(\"Creating and Z-ordering Delta table...\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(zordered_path)\n",
    "spark.sql(f\"OPTIMIZE delta.`{zordered_path}` ZORDER BY (user_id, content_id)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca4a43bc-b544-45ac-aede-67644e5d90dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting unoptimized Delta table...\nFound 19902 rows in 1095.26 ms\n\nTesting Z-ordered Delta table...\nFound 19902 rows in 703.11 ms\n\nZ-ordering improved performance by 35.80%\nZ-ordered query was 1.56x faster\n"
     ]
    }
   ],
   "source": [
    "# 5. Simple performance test\n",
    "import time\n",
    "\n",
    "# Test query that benefits from Z-ordering\n",
    "test_query = \"user_id LIKE 'USR1%' AND content_id LIKE 'CON1%'\"\n",
    "\n",
    "# Clear cache\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Test unoptimized\n",
    "print(\"\\nTesting unoptimized Delta table...\")\n",
    "start = time.time()\n",
    "unopt_count = spark.sql(f\"SELECT COUNT(*) FROM delta.`{unoptimized_path}` WHERE {test_query}\").collect()[0][0]\n",
    "unopt_time = (time.time() - start) * 1000\n",
    "print(f\"Found {unopt_count} rows in {unopt_time:.2f} ms\")\n",
    "\n",
    "# Clear cache again\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Test Z-ordered\n",
    "print(\"\\nTesting Z-ordered Delta table...\")\n",
    "start = time.time()\n",
    "zorder_count = spark.sql(f\"SELECT COUNT(*) FROM delta.`{zordered_path}` WHERE {test_query}\").collect()[0][0]\n",
    "zorder_time = (time.time() - start) * 1000\n",
    "print(f\"Found {zorder_count} rows in {zorder_time:.2f} ms\")\n",
    "\n",
    "# 6. Show comparison\n",
    "if zorder_time < unopt_time:\n",
    "    improvement = (unopt_time - zorder_time) / unopt_time * 100\n",
    "    print(f\"\\nZ-ordering improved performance by {improvement:.2f}%\")\n",
    "    print(f\"Z-ordered query was {unopt_time/zorder_time:.2f}x faster\")\n",
    "else:\n",
    "    print(\"\\nIn this demo, Z-ordering didn't show performance improvement.\")\n",
    "    print(\"This can happen with small datasets or cached data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc5ff13f-2513-470d-adff-2b7a473d7ae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Z-ordering Benefits\n",
    "print(\"Key benefits of Z-ordering:\")\n",
    "print(\"1. Multi-dimensional clustering - efficient for queries with multiple filter conditions\")\n",
    "print(\"2. Works well with high-cardinality columns (unlike partitioning)\")\n",
    "print(\"3. No need to decide in advance which columns to optimize (more flexible)\")\n",
    "print(\"4. Can be applied after table creation (unlike partitioning)\")\n",
    "print(\"5. Particularly effective for selective queries on large datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0d5106f-707b-455f-a7e7-9d2514ff30a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WRITING BEST PRACTICES\n",
    "print(\"Best practices when writing data:\")\n",
    "print(\"1. Choose the right file format for your use case\")\n",
    "print(\"   - Parquet/ORC: Good for analytics workloads\")\n",
    "print(\"   - Delta: When you need ACID transactions, time travel, schema enforcement\")\n",
    "print(\"2. Use appropriate partitioning for large datasets\")\n",
    "print(\"   - Partition on low-cardinality columns (date, country, category)\")\n",
    "print(\"   - Avoid over-partitioning (creates too many small files)\")\n",
    "print(\"3. Use Z-ordering for high-cardinality columns in Delta tables\")\n",
    "print(\"4. Consider compaction to manage file sizes\")\n",
    "print(\"5. Use appropriate write modes\")\n",
    "print(\"   - 'overwrite': Replace existing data\")\n",
    "print(\"   - 'append': Add to existing data\")\n",
    "print(\"   - 'ignore': Skip if data exists\")\n",
    "print(\"   - 'error': Fail if data exists (default)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Module 4, Clip 10: Writing Data to Various Destinations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}