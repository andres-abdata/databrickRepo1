{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46826f27-1a58-4e2d-9229-0c0f63ca30a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Using UDFs in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00e999b-6b38-4dfd-b431-47ab26ee7565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/Exercise2/\"))\n",
    "\n",
    "# Let's load our streaming events data\n",
    "file_path = \"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/Exercise2/complex_events.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ffd25d1-977f-4555-8834-4a73e9c27e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### USER DEFINED FUNCTIONS (UDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cfa91b2-8ce3-442a-acea-73582f75f7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UDFs allow us to apply custom Python functions to our data\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType\n",
    "\n",
    "# Let's create a simple UDF to calculate streaming quality score\n",
    "@udf(FloatType())\n",
    "def calculate_quality_score(duration, buffering_count):\n",
    "    if duration is None or buffering_count is None:\n",
    "        return None\n",
    "    \n",
    "    # Calculate a simple score where longer duration is good but buffering is bad\n",
    "    # Higher score is better\n",
    "    return (duration / 60.0) - (buffering_count * 10.0)\n",
    "\n",
    "# Apply our UDF to the DataFrame\n",
    "df_with_quality = df.withColumn(\"streaming_score\", \n",
    "                               calculate_quality_score(df[\"duration_seconds\"], df[\"buffering_count\"]))\n",
    "\n",
    "print(\"Data with streaming scores (Regular UDF):\")\n",
    "df_with_quality.select(\"duration_seconds\", \"buffering_count\", \"streaming_score\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5247820-06a2-4c75-8d44-9beb6f2a1060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UDFs can significantly slow down processing since they operate row by row\n",
    "# Let's check the execution plan to see the impact\n",
    "print(\"Execution plan with Regular UDF:\")\n",
    "print(\"--------------------------------\")\n",
    "df_with_quality.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f87812a-9e81-482f-b695-ff782a7cb784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### VECTORIZED UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "525db2e0-9e54-453a-a652-4d28680c293c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# VECTORIZED UDFs\n",
    "# Vectorized UDFs process data in batches using pandas, which dramatically improves performance\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Define a vectorized UDF for calculating the SAME streaming score\n",
    "# But using pandas for vectorized processing\n",
    "@pandas_udf(FloatType())\n",
    "def calculate_quality_score_vectorized(duration: pd.Series, buffering: pd.Series) -> pd.Series:\n",
    "    # Same logic as the regular UDF, but operates on entire Series at once\n",
    "    # This uses pandas' optimized C implementation for fast vector operations\n",
    "    return (duration / 60.0) - (buffering * 10.0)\n",
    "\n",
    "# Apply the vectorized UDF\n",
    "df_with_score_vectorized = df.withColumn(\"streaming_score\", \n",
    "                             calculate_quality_score_vectorized(df[\"duration_seconds\"], df[\"buffering_count\"]))\n",
    "\n",
    "print(\"Data with streaming scores (Vectorized UDF):\")\n",
    "df_with_score_vectorized.select(\"duration_seconds\", \"buffering_count\", \"streaming_score\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12de8493-a37d-482c-870e-833fef6e5903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's see the execution plan difference with vectorized UDF\n",
    "print(\"\\nExecution plan with Vectorized (Pandas) UDF:\")\n",
    "print(\"-------------------------------------------\")\n",
    "df_with_score_vectorized.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb0a57c5-43bd-4ce0-b314-9c9650267f61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Comparing performance between regular UDF and vectorized UDF\n",
    "import time\n",
    "\n",
    "#spark.config.set(\"spark.databricks.io.cache.enabled\",\"true\")\n",
    "\n",
    "# Force Spark to clear any previous caches or plans\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Time the regular UDF\n",
    "start_time = time.time()\n",
    "result1 = df_with_quality.select(\"streaming_score\")  # Use the DataFrame we already created with regular UDF\n",
    "result1.count()  # Force execution\n",
    "regular_time = time.time() - start_time\n",
    "print(f\"Regular UDF execution time: {regular_time:.2f} seconds\")\n",
    "\n",
    "# Time the vectorized UDF\n",
    "start_time = time.time()\n",
    "result2 = df_with_score_vectorized.select(\"streaming_score\")  # Use the DataFrame we already created with vectorized UDF\n",
    "result2.count()  # Force execution\n",
    "vectorized_time = time.time() - start_time\n",
    "print(f\"Vectorized UDF execution time: {vectorized_time:.2f} seconds\")\n",
    "\n",
    "print(f\"Speedup factor: {regular_time/vectorized_time:.2f}x faster with vectorized UDF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59271518-54bb-4769-8e1e-f519cadf4882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Key differences in the explain output:\n",
    "# 1. \"ArrowEvalPython\" instead of \"BatchEvalPython\" - uses Apache Arrow for efficient data transfer\n",
    "# 2. Fewer conversion operations than regular UDFs\n",
    "# 3. Better integration with Spark's optimization engine\n",
    "# Note: Both UDF types still break Photon optimization, but vectorized UDFs are much more efficient"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Module 3, Clip 8 - Using User Defined Functions in PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
