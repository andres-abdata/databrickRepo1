{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46826f27-1a58-4e2d-9229-0c0f63ca30a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Using UDFs in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00e999b-6b38-4dfd-b431-47ab26ee7565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's load our streaming events data\n",
    "file_path = \"/pyspark/video-streaming-data/module3-transform/optimization/complex_events.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ffd25d1-977f-4555-8834-4a73e9c27e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### USER DEFINED FUNCTIONS (UDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cfa91b2-8ce3-442a-acea-73582f75f7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with streaming scores (Regular UDF):\n+----------------+---------------+---------------+\n|duration_seconds|buffering_count|streaming_score|\n+----------------+---------------+---------------+\n|             565|              4|     -30.583334|\n|            2018|              1|      23.633333|\n|            2900|              3|      18.333334|\n|            3242|              3|      24.033333|\n|            4248|              1|           60.8|\n+----------------+---------------+---------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# UDFs allow us to apply custom Python functions to our data\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType\n",
    "\n",
    "# Let's create a simple UDF to calculate streaming quality score\n",
    "@udf(FloatType())\n",
    "def calculate_quality_score(duration, buffering_count):\n",
    "    if duration is None or buffering_count is None:\n",
    "        return None\n",
    "    \n",
    "    # Calculate a simple score where longer duration is good but buffering is bad\n",
    "    # Higher score is better\n",
    "    return (duration / 60.0) - (buffering_count * 10.0)\n",
    "\n",
    "# Apply our UDF to the DataFrame\n",
    "df_with_quality = df.withColumn(\"streaming_score\", \n",
    "                               calculate_quality_score(df[\"duration_seconds\"], df[\"buffering_count\"]))\n",
    "\n",
    "print(\"Data with streaming scores (Regular UDF):\")\n",
    "df_with_quality.select(\"duration_seconds\", \"buffering_count\", \"streaming_score\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5247820-06a2-4c75-8d44-9beb6f2a1060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution plan with Regular UDF:\n--------------------------------\n== Physical Plan ==\n* Project (3)\n+- BatchEvalPython (2)\n   +- Scan csv  (1)\n\n\n(1) Scan csv \nOutput [12]: [event_id#23, user_id#24, content_id#25, timestamp#26, duration_seconds#27, device_type#28, quality#29, buffering_count#30, error_type#31, ip_address#32, country#33, session_id#34]\nBatched: false\nLocation: InMemoryFileIndex [dbfs:/pyspark/video-streaming-data/module3-transform/optimization/complex_events.csv]\nReadSchema: struct<event_id:string,user_id:string,content_id:string,timestamp:timestamp,duration_seconds:int,device_type:string,quality:string,buffering_count:int,error_type:string,ip_address:string,country:string,session_id:string>\n\n(2) BatchEvalPython\nInput [12]: [event_id#23, user_id#24, content_id#25, timestamp#26, duration_seconds#27, device_type#28, quality#29, buffering_count#30, error_type#31, ip_address#32, country#33, session_id#34]\nArguments: [calculate_quality_score(duration_seconds#27, buffering_count#30)#47], [pythonUDF0#82]\n\n(3) Project [codegen id : 1]\nOutput [13]: [event_id#23, user_id#24, content_id#25, timestamp#26, duration_seconds#27, device_type#28, quality#29, buffering_count#30, error_type#31, ip_address#32, country#33, session_id#34, pythonUDF0#82 AS streaming_score#48]\nInput [13]: [event_id#23, user_id#24, content_id#25, timestamp#26, duration_seconds#27, device_type#28, quality#29, buffering_count#30, error_type#31, ip_address#32, country#33, session_id#34, pythonUDF0#82]\n\n\n"
     ]
    }
   ],
   "source": [
    "# UDFs can significantly slow down processing since they operate row by row\n",
    "# Let's check the execution plan to see the impact\n",
    "print(\"Execution plan with Regular UDF:\")\n",
    "print(\"--------------------------------\")\n",
    "df_with_quality.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f87812a-9e81-482f-b695-ff782a7cb784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### VECTORIZED UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "525db2e0-9e54-453a-a652-4d28680c293c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with streaming scores (Vectorized UDF):\n+----------------+---------------+---------------+\n|duration_seconds|buffering_count|streaming_score|\n+----------------+---------------+---------------+\n|             565|              4|     -30.583334|\n|            2018|              1|      23.633333|\n|            2900|              3|      18.333334|\n|            3242|              3|      24.033333|\n|            4248|              1|           60.8|\n+----------------+---------------+---------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# VECTORIZED UDFs\n",
    "# Vectorized UDFs process data in batches using pandas, which dramatically improves performance\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Define a vectorized UDF for calculating the SAME streaming score\n",
    "# But using pandas for vectorized processing\n",
    "@pandas_udf(FloatType())\n",
    "def calculate_quality_score_vectorized(duration: pd.Series, buffering: pd.Series) -> pd.Series:\n",
    "    # Same logic as the regular UDF, but operates on entire Series at once\n",
    "    # This uses pandas' optimized C implementation for fast vector operations\n",
    "    return (duration / 60.0) - (buffering * 10.0)\n",
    "\n",
    "# Apply the vectorized UDF\n",
    "df_with_score_vectorized = df.withColumn(\"streaming_score\", \n",
    "                             calculate_quality_score_vectorized(df[\"duration_seconds\"], df[\"buffering_count\"]))\n",
    "\n",
    "print(\"Data with streaming scores (Vectorized UDF):\")\n",
    "df_with_score_vectorized.select(\"duration_seconds\", \"buffering_count\", \"streaming_score\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12de8493-a37d-482c-870e-833fef6e5903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nExecution plan with Vectorized (Pandas) UDF:\n-------------------------------------------\n== Physical Plan ==\n* Project (3)\n+- ArrowEvalPython (2)\n   +- Scan csv  (1)\n\n\n(1) Scan csv \nOutput [12]: [event_id#23, user_id#24, content_id#25, timestamp#26, duration_seconds#27, device_type#28, quality#29, buffering_count#30, error_type#31, ip_address#32, country#33, session_id#34]\nBatched: false\nLocation: InMemoryFileIndex [dbfs:/pyspark/video-streaming-data/module3-transform/optimization/complex_events.csv]\nReadSchema: struct<event_id:string,user_id:string,content_id:string,timestamp:timestamp,duration_seconds:int,device_type:string,quality:string,buffering_count:int,error_type:string,ip_address:string,country:string,session_id:string>\n\n(2) ArrowEvalPython\nInput [12]: [event_id#23, user_id#24, content_id#25, timestamp#26, duration_seconds#27, device_type#28, quality#29, buffering_count#30, error_type#31, ip_address#32, country#33, session_id#34]\nArguments: [calculate_quality_score_vectorized(duration_seconds#27, buffering_count#30)#83], [pythonUDF0#117], 200\n\n(3) Project [codegen id : 1]\nOutput [13]: [event_id#23, user_id#24, content_id#25, timestamp#26, duration_seconds#27, device_type#28, quality#29, buffering_count#30, error_type#31, ip_address#32, country#33, session_id#34, pythonUDF0#117 AS streaming_score#84]\nInput [13]: [event_id#23, user_id#24, content_id#25, timestamp#26, duration_seconds#27, device_type#28, quality#29, buffering_count#30, error_type#31, ip_address#32, country#33, session_id#34, pythonUDF0#117]\n\n\n"
     ]
    }
   ],
   "source": [
    "# Let's see the execution plan difference with vectorized UDF\n",
    "print(\"\\nExecution plan with Vectorized (Pandas) UDF:\")\n",
    "print(\"-------------------------------------------\")\n",
    "df_with_score_vectorized.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb0a57c5-43bd-4ce0-b314-9c9650267f61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular UDF execution time: 2.15 seconds\nVectorized UDF execution time: 0.55 seconds\nSpeedup factor: 3.90x faster with vectorized UDF\n"
     ]
    }
   ],
   "source": [
    "# Comparing performance between regular UDF and vectorized UDF\n",
    "import time\n",
    "\n",
    "# Force Spark to clear any previous caches or plans\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Time the regular UDF\n",
    "start_time = time.time()\n",
    "result1 = df_with_quality.select(\"streaming_score\")  # Use the DataFrame we already created with regular UDF\n",
    "result1.count()  # Force execution\n",
    "regular_time = time.time() - start_time\n",
    "print(f\"Regular UDF execution time: {regular_time:.2f} seconds\")\n",
    "\n",
    "# Time the vectorized UDF\n",
    "start_time = time.time()\n",
    "result2 = df_with_score_vectorized.select(\"streaming_score\")  # Use the DataFrame we already created with vectorized UDF\n",
    "result2.count()  # Force execution\n",
    "vectorized_time = time.time() - start_time\n",
    "print(f\"Vectorized UDF execution time: {vectorized_time:.2f} seconds\")\n",
    "\n",
    "print(f\"Speedup factor: {regular_time/vectorized_time:.2f}x faster with vectorized UDF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59271518-54bb-4769-8e1e-f519cadf4882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Key differences in the explain output:\n",
    "# 1. \"ArrowEvalPython\" instead of \"BatchEvalPython\" - uses Apache Arrow for efficient data transfer\n",
    "# 2. Fewer conversion operations than regular UDFs\n",
    "# 3. Better integration with Spark's optimization engine\n",
    "# Note: Both UDF types still break Photon optimization, but vectorized UDFs are much more efficient"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Module 3, Clip 8 - Using User Defined Functions in PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}