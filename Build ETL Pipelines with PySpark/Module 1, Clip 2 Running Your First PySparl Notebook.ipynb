{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66aa4610-58c5-44c4-86e0-b8a6969f8549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Running Your First PySpark Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92005119-b3c4-4b25-99dc-0aa7c3970b29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In Databricks, the SparkSession is already available as 'spark'\n",
    "\n",
    "# Let's start by creating a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# But in a regular environment, you would create it like this:\n",
    "spark = SparkSession.builder \\\n",
    "     .appName(\"FirstPySparkJob\") \\\n",
    "     .getOrCreate()\n",
    "\n",
    "print(\"Spark session is active and ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6621620-15cc-41eb-9296-468ac87909ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(dbutils.fs.ls(\"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/Exercise2/\"))\n",
    "\n",
    "\n",
    "# Let's load a sample dataset from our video streaming data\n",
    "# This is a small sample to demonstrate basic PySpark operations\n",
    "file_path = \"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/Exercise2/streaming_sample_10k.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "df.limit(500).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f920c694-b5d4-41c9-816b-10d5bd30fcea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's see the first few rows of our data\n",
    "print(\"Sample data:\")\n",
    "display(df.limit(5))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd777aa-c3b9-43f0-b610-f42a7c51c41d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now, let's perform some basic operations\n",
    "# Count the total number of records\n",
    "total_records = df.count()\n",
    "print(f\"Total number of streaming events: {total_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ad6637b-cd5b-4790-be4c-fbd32e31c102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_grouped = (\n",
    "    df.groupBy('device_type')\n",
    "    .agg(F.count('event_id').alias('total_event'))\n",
    "    .orderBy(F.desc('device_type'))\n",
    ")\n",
    "\n",
    "df_grouped.limit(500).display()\n",
    "\n",
    "\n",
    "# # Get a summary of the different device types\n",
    "# device_counts = df.groupBy(\"device_type\").count().orderBy(\"count\", ascending=False)\n",
    "# print(\"Streaming events by device type:\")\n",
    "# display(device_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab00cdf5-e3ef-4e59-ad79-4a907a9896dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df.createOrReplaceTempView(\"streaming_events\")\n",
    "\n",
    "query_result = spark.sql(\"\"\"\n",
    "  SELECT country, COUNT(*) as event_count\n",
    "  FROM streaming_events\n",
    "  GROUP BY country\n",
    "  ORDER BY event_count DESC                         \n",
    "\"\"\")\n",
    "\n",
    "print('Streaming events by country: ')\n",
    "display(query_result)\n",
    "\n",
    "\n",
    "# # Create a temporary view of our DataFrame for SQL\n",
    "# df.createOrReplaceTempView(\"streaming_events\")\n",
    "\n",
    "# # Execute SQL query using spark.sql()\n",
    "# query_result = spark.sql(\"\"\"\n",
    "# SELECT \n",
    "#   country, \n",
    "#   COUNT(*) as event_count\n",
    "# FROM streaming_events\n",
    "# GROUP BY country\n",
    "# ORDER BY event_count DESC\n",
    "# \"\"\")\n",
    "\n",
    "# print(\"Streaming events by country:\")\n",
    "# display(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b985e40b-7839-4505-a7df-1206bb87742a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_path = 'abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/Exercise2/Export/processed_data'\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "# # Save the processed data to a new location\n",
    "# output_path = \"/pyspark/video-streaming-data/module1-intro/first_job/processed_data\"\n",
    "\n",
    "# # Save as Parquet (a columnar storage format that's efficient for analytics)\n",
    "# df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "# print(f\"Data successfully saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7536573881796008,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Module 1, Clip 2 Running Your First PySparl Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
