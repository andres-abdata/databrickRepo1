{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83c44eac-ac53-4779-9321-a9f703afb7dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Implementing Incremental Updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b95b3cb-35e8-45ac-aa23-e23702b63b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#display(dbutils.fs.ls(\"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/VideoStreamingData/\"))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, concat, when\n",
    "from delta.tables import DeltaTable\n",
    "import datetime\n",
    "\n",
    "# Define a timestamped Delta table path for a fresh run each time\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "delta_table_path = f\"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/VideoStreamingData/delta_events_{timestamp}\"\n",
    "print(f\"Using new Delta table path: {delta_table_path}\")\n",
    "\n",
    "# Create initial data instead of reading existing data\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Create a simple schema for this demo\n",
    "schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"content_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"duration_seconds\", IntegerType(), True),\n",
    "    StructField(\"device_type\", StringType(), True),\n",
    "    StructField(\"quality\", StringType(), True),\n",
    "    StructField(\"buffering_count\", IntegerType(), True),\n",
    "    StructField(\"error_type\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create sample data\n",
    "data = [\n",
    "    (\"EVT10001\", \"USR10123\", \"CON10456\", datetime.datetime.now(), 3600, \"TV\", \"HD\", 0, None),\n",
    "    (\"EVT10002\", \"USR10124\", \"CON10457\", datetime.datetime.now(), 1800, \"Mobile\", \"SD\", 2, None),\n",
    "    (\"EVT10003\", \"USR10125\", \"CON10458\", datetime.datetime.now(), 2400, \"Web\", \"4K\", 1, None),\n",
    "    (\"EVT10004\", \"USR10126\", \"CON10459\", datetime.datetime.now(), 5400, \"TV\", \"4K\", 0, None),\n",
    "    (\"EVT10005\", \"USR10127\", \"CON10460\", datetime.datetime.now(), 1200, \"Tablet\", \"HD\", 3, \"network_error\")\n",
    "]\n",
    "\n",
    "# Create our baseline DataFrame\n",
    "existing_df = spark.createDataFrame(data, schema)\n",
    "print(f\"Created sample data with {existing_df.count()} records\")\n",
    "\n",
    "# Write to Delta format to initialize our table\n",
    "existing_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "print(\"Created initial Delta table with sample data\")\n",
    "\n",
    "\n",
    "# Let's first examine the existing Delta table schema to ensure we match it exactly\n",
    "print(\"Existing Delta table schema:\")\n",
    "existing_delta = spark.read.format(\"delta\").load(delta_table_path)\n",
    "existing_delta.printSchema()\n",
    "\n",
    "# Get a sample of the existing data\n",
    "print(\"Sample of existing data:\")\n",
    "existing_delta.limit(5).display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db45d332-6e6e-4e95-b61a-edc028b4d98c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### METHOD 1: APPEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de3f2d9-ee14-44be-896d-7054d7d1cf59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ial_"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "initial_count = spark.read.format('delta').load(delta_table_path).count()\n",
    "print(initial_count)\n",
    "\n",
    "sample_data = existing_delta.limit(2)\n",
    "display(sample_data)\n",
    "\n",
    "new_records_df = sample_data.withColumn(\"event_id\", concat(lit(\"APEND_\"), col('event_id')))\n",
    "display(new_records_df)\n",
    "\n",
    "#Append new data\n",
    "new_records_df.write.format('delta').mode('append').save(delta_table_path)\n",
    "\n",
    "# # Get updated count\n",
    "updated_count = spark.read.format(\"delta\").load(delta_table_path).count()\n",
    "print(f\"Updated record count: {updated_count}\")\n",
    "print(f\"Added {updated_count - initial_count} records\")\n",
    "\n",
    "\n",
    "# # Get initial count\n",
    "# initial_count = spark.read.format(\"delta\").load(delta_table_path).count()\n",
    "# print(f\"Initial record count: {initial_count}\")\n",
    "\n",
    "# # Take an existing record and modify it for the append demo\n",
    "# sample_records = existing_delta.limit(2)\n",
    "# new_records_df = sample_records.withColumn(\"event_id\", concat(lit(\"APPEND_\"), col(\"event_id\")))\n",
    "\n",
    "# # Show the new records\n",
    "# print(\"New records to append:\")\n",
    "# new_records_df.display()\n",
    "\n",
    "# # Append new data\n",
    "# new_records_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "\n",
    "# # Get updated count\n",
    "# updated_count = spark.read.format(\"delta\").load(delta_table_path).count()\n",
    "# print(f\"Updated record count: {updated_count}\")\n",
    "# print(f\"Added {updated_count - initial_count} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af990924-157a-4db5-83c8-3b8145be927d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(delta_table_path).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa0cdce6-f318-425d-8ea6-4d5165803b3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### METHOD 2: MERGE (UPSERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a0bf793-b8f9-4aaa-91c3-a3325c3d8a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Overwrite the existing data to reset the table for this Method\n",
    "existing_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "print(\"Created initial Delta table with sample data\")\n",
    "existing_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b7bec10-5f31-4d4c-b620-35358a9d9546",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"event_id\":165},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753240746323}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get a record to update and a record to insert\n",
    "records_to_process = existing_delta.limit(2)\n",
    "record_to_update = records_to_process.limit(1)\n",
    "record_to_insert = records_to_process.limit(1).withColumn(\"event_id\", lit(\"MERGE_NEW_RECORD\"))\n",
    "\n",
    "record_to_update.display()\n",
    "record_to_insert.display()\n",
    "\n",
    "# Modify the record to update\n",
    "update_id = record_to_update.select(\"event_id\").collect()[0][0]\n",
    "print(f\"Event ID to update: {update_id}\")\n",
    "\n",
    "update_df = record_to_update.withColumn(\"duration_seconds\", lit(9999))\n",
    "update_df = update_df.withColumn(\"quality\", lit(\"SUPER-HD\"))\n",
    "\n",
    "# Combine for the merge operation\n",
    "incremental_df = update_df.union(record_to_insert)\n",
    "print(\"Records for merge operation:\")\n",
    "incremental_df.display()\n",
    "\n",
    "# Perform MERGE operation\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "# Check which columns exist in both dataframes to ensure a clean merge\n",
    "target_columns = set(spark.read.format(\"delta\").load(delta_table_path).columns)\n",
    "source_columns = set(incremental_df.columns)\n",
    "common_updatable_cols = target_columns.intersection(source_columns) - {\"event_id\"}\n",
    "\n",
    "# Build the update dictionary dynamically based on common columns\n",
    "update_dict = {col_name: f\"source.{col_name}\" for col_name in common_updatable_cols}\n",
    "\n",
    "# Perform the merge\n",
    "delta_table.alias(\"target\").merge(\n",
    "    incremental_df.alias(\"source\"),\n",
    "    f\"target.event_id = source.event_id\"\n",
    ").whenMatchedUpdate(\n",
    "    set=update_dict\n",
    ").whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "256caa16-0039-4e74-b844-ffca0ccb44cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(delta_table_path).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "695a5a86-2033-4133-80fe-7b19789c1314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### METHOD 3: SCHEMA EVOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f9f6396-fa92-449c-aa1b-44c39beab8ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Overwrite the existing data to reset the table for this Method\n",
    "existing_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "# View the current schema\n",
    "existing_df.printSchema(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45bbd17a-ed4f-47b8-922e-9f271e45d262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's create data with a new column\n",
    "# Get a few records to evolve\n",
    "base_records = spark.read.format(\"delta\").load(delta_table_path).limit(2)\n",
    "\n",
    "# Add a new column\n",
    "evolved_df = base_records.withColumn(\"user_rating\", lit(4.5))\n",
    "evolved_df = evolved_df.withColumn(\"event_id\", concat(lit(\"EVOLVED_\"), col(\"event_id\")))\n",
    "\n",
    "print(\"Records with new schema:\")\n",
    "evolved_df.printSchema()\n",
    "evolved_df.display()\n",
    "\n",
    "# Write with mergeSchema option\n",
    "evolved_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea271b2e-11cb-433f-ba5a-b09ca59ec4f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(delta_table_path).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Module 4, Clip 11_ Incremental Data Updates with PySpark and Delta Lake",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
