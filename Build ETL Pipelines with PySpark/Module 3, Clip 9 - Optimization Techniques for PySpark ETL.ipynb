{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46826f27-1a58-4e2d-9229-0c0f63ca30a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optimization Techniques for PySpark ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85fa911a-b8d8-44c3-945e-a425dbf59c60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load our data\n",
    "file_path = \"/pyspark/video-streaming-data/module3-transform/optimization/complex_events.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "user_path = \"/pyspark/video-streaming-data/module3-transform/joins_aggregations/users.csv\"\n",
    "users_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(user_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31aeeeff-1ca4-4d24-9915-29e8c105e990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### OPTIMIZATION 1: BROADCAST JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9aeee4-b2aa-4b00-94d5-2a86f152d8e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabling automatic broadcast join optimization...\n\n1. REGULAR JOIN WITH SHUFFLE:\n-----------------------------\nExecution plan for regular join:\n== Physical Plan ==\nAdaptiveSparkPlan (11)\n+- == Initial Plan ==\n   Project (10)\n   +- SortMergeJoin Inner (9)\n      :- Sort (4)\n      :  +- Exchange (3)\n      :     +- Filter (2)\n      :        +- Scan csv  (1)\n      +- Sort (8)\n         +- Exchange (7)\n            +- Filter (6)\n               +- Scan csv  (5)\n\n\n(1) Scan csv \nOutput [12]: [event_id#149, user_id#150, content_id#151, timestamp#152, duration_seconds#153, device_type#154, quality#155, buffering_count#156, error_type#157, ip_address#158, country#159, session_id#160]\nBatched: false\nLocation: InMemoryFileIndex [dbfs:/pyspark/video-streaming-data/module3-transform/optimization/complex_events.csv]\nPushedFilters: [IsNotNull(user_id)]\nReadSchema: struct<event_id:string,user_id:string,content_id:string,timestamp:timestamp,duration_seconds:int,device_type:string,quality:string,buffering_count:int,error_type:string,ip_address:string,country:string,session_id:string>\n\n(2) Filter\nInput [12]: [event_id#149, user_id#150, content_id#151, timestamp#152, duration_seconds#153, device_type#154, quality#155, buffering_count#156, error_type#157, ip_address#158, country#159, session_id#160]\nCondition : isnotnull(user_id#150)\n\n(3) Exchange\nInput [12]: [event_id#149, user_id#150, content_id#151, timestamp#152, duration_seconds#153, device_type#154, quality#155, buffering_count#156, error_type#157, ip_address#158, country#159, session_id#160]\nArguments: hashpartitioning(user_id#150, 200), ENSURE_REQUIREMENTS, [plan_id=281]\n\n(4) Sort\nInput [12]: [event_id#149, user_id#150, content_id#151, timestamp#152, duration_seconds#153, device_type#154, quality#155, buffering_count#156, error_type#157, ip_address#158, country#159, session_id#160]\nArguments: [user_id#150 ASC NULLS FIRST], false, 0\n\n(5) Scan csv \nOutput [9]: [user_id#190, signup_date#191, subscription_tier#192, last_billing_date#193, account_status#194, preferred_genres#195, age_group#196, gender#197, language#198]\nBatched: false\nLocation: InMemoryFileIndex [dbfs:/pyspark/video-streaming-data/module3-transform/joins_aggregations/users.csv]\nPushedFilters: [IsNotNull(user_id)]\nReadSchema: struct<user_id:string,signup_date:date,subscription_tier:string,last_billing_date:date,account_status:string,preferred_genres:string,age_group:string,gender:string,language:string>\n\n(6) Filter\nInput [9]: [user_id#190, signup_date#191, subscription_tier#192, last_billing_date#193, account_status#194, preferred_genres#195, age_group#196, gender#197, language#198]\nCondition : isnotnull(user_id#190)\n\n(7) Exchange\nInput [9]: [user_id#190, signup_date#191, subscription_tier#192, last_billing_date#193, account_status#194, preferred_genres#195, age_group#196, gender#197, language#198]\nArguments: hashpartitioning(user_id#190, 200), ENSURE_REQUIREMENTS, [plan_id=282]\n\n(8) Sort\nInput [9]: [user_id#190, signup_date#191, subscription_tier#192, last_billing_date#193, account_status#194, preferred_genres#195, age_group#196, gender#197, language#198]\nArguments: [user_id#190 ASC NULLS FIRST], false, 0\n\n(9) SortMergeJoin\nLeft keys [1]: [user_id#150]\nRight keys [1]: [user_id#190]\nJoin type: Inner\nJoin condition: None\n\n(10) Project\nOutput [20]: [user_id#150, event_id#149, content_id#151, timestamp#152, duration_seconds#153, device_type#154, quality#155, buffering_count#156, error_type#157, ip_address#158, country#159, session_id#160, signup_date#191, subscription_tier#192, last_billing_date#193, account_status#194, preferred_genres#195, age_group#196, gender#197, language#198]\nInput [21]: [event_id#149, user_id#150, content_id#151, timestamp#152, duration_seconds#153, device_type#154, quality#155, buffering_count#156, error_type#157, ip_address#158, country#159, session_id#160, user_id#190, signup_date#191, subscription_tier#192, last_billing_date#193, account_status#194, preferred_genres#195, age_group#196, gender#197, language#198]\n\n(11) AdaptiveSparkPlan\nOutput [20]: [user_id#150, event_id#149, content_id#151, timestamp#152, duration_seconds#153, device_type#154, quality#155, buffering_count#156, error_type#157, ip_address#158, country#159, session_id#160, signup_date#191, subscription_tier#192, last_billing_date#193, account_status#194, preferred_genres#195, age_group#196, gender#197, language#198]\nArguments: isFinalPlan=false\n\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Regular join (will use shuffle)\n",
    "# First, let's disable automatic broadcasting to clearly show the difference\n",
    "print(\"Disabling automatic broadcast join optimization...\")\n",
    "original_broadcast_threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")  # Disable automatic broadcasting\n",
    "\n",
    "# Clear everything to ensure a clean demonstration\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# 1. Regular join (will use shuffle)\n",
    "print(\"\\n1. REGULAR JOIN WITH SHUFFLE:\")\n",
    "print(\"-----------------------------\")\n",
    "regular_join = df.join(users_df, on=\"user_id\")\n",
    "print(\"Execution plan for regular join:\")\n",
    "regular_join.explain(mode=\"formatted\")\n",
    "\n",
    "# Look for Exchange hashpartitioning in the plan - this indicates shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6597b88c-8b3f-4677-beeb-f8932d895ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n2. BROADCAST JOIN:\n-----------------\nExecution plan for broadcast join:\n== Physical Plan ==\nAdaptiveSparkPlan (8)\n+- == Initial Plan ==\n   Project (7)\n   +- BroadcastHashJoin Inner BuildRight (6)\n      :- Filter (2)\n      :  +- Scan csv  (1)\n      +- Exchange (5)\n         +- Filter (4)\n            +- Scan csv  (3)\n\n\n(1) Scan csv \nOutput [12]: [event_id#149, user_id#150, content_id#151, timestamp#152, duration_seconds#153, device_type#154, quality#155, buffering_count#156, error_type#157, ip_address#158, country#159, session_id#160]\nBatched: false\nLocation: InMemoryFileIndex [dbfs:/pyspark/video-streaming-data/module3-transform/optimization/complex_events.csv]\nPushedFilters: [IsNotNull(user_id)]\nReadSchema: struct<event_id:string,user_id:string,content_id:string,timestamp:timestamp,duration_seconds:int,device_type:string,quality:string,buffering_count:int,error_type:string,ip_address:string,country:string,session_id:string>\n\n(2) Filter\nInput [12]: [event_id#149, user_id#150, content_id#151, timestamp#152, duration_seconds#153, device_type#154, quality#155, buffering_count#156, error_type#157, ip_address#158, country#159, session_id#160]\nCondition : isnotnull(user_id#150)\n\n(3) Scan csv \nOutput [9]: [user_id#190, signup_date#191, subscription_tier#192, last_billing_date#193, account_status#194, preferred_genres#195, age_group#196, gender#197, language#198]\nBatched: false\nLocation: InMemoryFileIndex [dbfs:/pyspark/video-streaming-data/module3-transform/joins_aggregations/users.csv]\nPushedFilters: [IsNotNull(user_id)]\nReadSchema: struct<user_id:string,signup_date:date,subscription_tier:string,last_billing_date:date,account_status:string,preferred_genres:string,age_group:string,gender:string,language:string>\n\n(4) Filter\nInput [9]: [user_id#190, signup_date#191, subscription_tier#192, last_billing_date#193, account_status#194, preferred_genres#195, age_group#196, gender#197, language#198]\nCondition : isnotnull(user_id#190)\n\n(5) Exchange\nInput [9]: [user_id#190, signup_date#191, subscription_tier#192, last_billing_date#193, account_status#194, preferred_genres#195, age_group#196, gender#197, language#198]\nArguments: SinglePartition, EXECUTOR_BROADCAST, [plan_id=333]\n\n(6) BroadcastHashJoin\nLeft keys [1]: [user_id#150]\nRight keys [1]: [user_id#190]\nJoin type: Inner\nJoin condition: None\n\n(7) Project\nOutput [20]: [user_id#150, event_id#149, content_id#151, timestamp#152, duration_seconds#153, device_type#154, quality#155, buffering_count#156, error_type#157, ip_address#158, country#159, session_id#160, signup_date#191, subscription_tier#192, last_billing_date#193, account_status#194, preferred_genres#195, age_group#196, gender#197, language#198]\nInput [21]: [event_id#149, user_id#150, content_id#151, timestamp#152, duration_seconds#153, device_type#154, quality#155, buffering_count#156, error_type#157, ip_address#158, country#159, session_id#160, user_id#190, signup_date#191, subscription_tier#192, last_billing_date#193, account_status#194, preferred_genres#195, age_group#196, gender#197, language#198]\n\n(8) AdaptiveSparkPlan\nOutput [20]: [user_id#150, event_id#149, content_id#151, timestamp#152, duration_seconds#153, device_type#154, quality#155, buffering_count#156, error_type#157, ip_address#158, country#159, session_id#160, signup_date#191, subscription_tier#192, last_billing_date#193, account_status#194, preferred_genres#195, age_group#196, gender#197, language#198]\nArguments: isFinalPlan=false\n\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Explicit broadcast join\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "print(\"\\n2. BROADCAST JOIN:\")\n",
    "print(\"-----------------\")\n",
    "broadcast_join = df.join(broadcast(users_df), on=\"user_id\")\n",
    "print(\"Execution plan for broadcast join:\")\n",
    "broadcast_join.explain(mode=\"formatted\")\n",
    "\n",
    "# Look for BroadcastExchange or BroadcastHashJoin in the plan - this indicates broadcasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1be68ea-d500-49c9-829a-1dca7c849604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nComparing performance:\n---------------------\nRegular join time: 2.65 seconds for 5054 records\nBroadcast join time: 0.97 seconds for 5054 records\nSpeedup: 2.72x faster with broadcast join\n\nRestored broadcast threshold to original value: -1\n"
     ]
    }
   ],
   "source": [
    "# Compare query times\n",
    "import time\n",
    "\n",
    "print(\"\\nComparing performance:\")\n",
    "print(\"---------------------\")\n",
    "\n",
    "# Time the regular join\n",
    "start_time = time.time()\n",
    "regular_count = regular_join.count()\n",
    "regular_time = time.time() - start_time\n",
    "print(f\"Regular join time: {regular_time:.2f} seconds for {regular_count} records\")\n",
    "\n",
    "# Time the broadcast join\n",
    "start_time = time.time()\n",
    "broadcast_count = broadcast_join.count()\n",
    "broadcast_time = time.time() - start_time\n",
    "print(f\"Broadcast join time: {broadcast_time:.2f} seconds for {broadcast_count} records\")\n",
    "\n",
    "print(f\"Speedup: {regular_time/broadcast_time:.2f}x faster with broadcast join\")\n",
    "\n",
    "# Restore original broadcast threshold\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", original_broadcast_threshold)\n",
    "print(f\"\\nRestored broadcast threshold to original value: {original_broadcast_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4d3a78a-391b-430d-93a5-77e9bda7b107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### OPTIMIZATION 2: CACHING STRATEGIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc3476fa-5bae-4551-b0fe-027d2b6cdf1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching demonstration:\n+-----------+-----+\n|device_type|count|\n+-----------+-----+\n|         TV|12550|\n|     Mobile|12328|\n|     Tablet|12461|\n|        Web|12661|\n+-----------+-----+\n\n+-----------+---------------------+\n|device_type|avg(duration_seconds)|\n+-----------+---------------------+\n|         TV|    4352.044302788845|\n|     Mobile|   4258.6459279688515|\n|     Tablet|    4335.961399566648|\n|        Web|    4215.865808387963|\n+-----------+---------------------+\n\nNon-cached execution time: 2.62 seconds\n+-----------+-----+\n|device_type|count|\n+-----------+-----+\n|         TV|12550|\n|     Mobile|12328|\n|     Tablet|12461|\n|        Web|12661|\n+-----------+-----+\n\n+-----------+---------------------+\n|device_type|avg(duration_seconds)|\n+-----------+---------------------+\n|         TV|    4352.044302788845|\n|     Mobile|   4258.6459279688515|\n|     Tablet|    4335.961399566648|\n|        Web|    4215.865808387963|\n+-----------+---------------------+\n\nCached execution time: 1.05 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[event_id: string, user_id: string, content_id: string, timestamp: timestamp, duration_seconds: int, device_type: string, quality: string, buffering_count: int, error_type: string, ip_address: string, country: string, session_id: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache intermediate results that are used multiple times\n",
    "import time\n",
    "\n",
    "print(\"Caching demonstration:\")\n",
    "# First, clear any existing cache\n",
    "df.unpersist(blocking=True)\n",
    "\n",
    "# Non-cached approach\n",
    "start_time = time.time()\n",
    "result1 = df.groupBy(\"device_type\").count()\n",
    "result1.show()\n",
    "\n",
    "result2 = df.groupBy(\"device_type\").agg({\"duration_seconds\": \"mean\"})\n",
    "result2.show()\n",
    "end_time = time.time()\n",
    "print(f\"Non-cached execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Cached approach\n",
    "df.cache() # Cache the dataframe\n",
    "df.count() # Force cache evaluation\n",
    "start_time = time.time()\n",
    "result1 = df.groupBy(\"device_type\").count()\n",
    "result1.show()\n",
    "\n",
    "result2 = df.groupBy(\"device_type\").agg({\"duration_seconds\": \"mean\"})\n",
    "result2.show()\n",
    "end_time = time.time()\n",
    "print(f\"Cached execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Remember to unpersist when done\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c806b8-3229-4150-9135-1bb713b7c90b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### OPTIMIZATION 3: PARTITIONED WRITES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c7af2b-235c-4837-afa9-80bde95da0b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned Writes Demonstration:\n--------------------------------\nWriting data partitioned by device_type...\n\nPartition structure created at /pyspark/video-streaming-data/module3-transform/optimization/optimized_output:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/pyspark/video-streaming-data/module3-transform/optimization/optimized_output/_SUCCESS</td><td>_SUCCESS</td><td>0</td><td>1744664659000</td></tr><tr><td>dbfs:/pyspark/video-streaming-data/module3-transform/optimization/optimized_output/_committed_6586057143507670899</td><td>_committed_6586057143507670899</td><td>35</td><td>1744499442000</td></tr><tr><td>dbfs:/pyspark/video-streaming-data/module3-transform/optimization/optimized_output/device_type=Mobile/</td><td>device_type=Mobile/</td><td>0</td><td>1744497246000</td></tr><tr><td>dbfs:/pyspark/video-streaming-data/module3-transform/optimization/optimized_output/device_type=TV/</td><td>device_type=TV/</td><td>0</td><td>1744497246000</td></tr><tr><td>dbfs:/pyspark/video-streaming-data/module3-transform/optimization/optimized_output/device_type=Tablet/</td><td>device_type=Tablet/</td><td>0</td><td>1744497247000</td></tr><tr><td>dbfs:/pyspark/video-streaming-data/module3-transform/optimization/optimized_output/device_type=Web/</td><td>device_type=Web/</td><td>0</td><td>1744497246000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/pyspark/video-streaming-data/module3-transform/optimization/optimized_output/_SUCCESS",
         "_SUCCESS",
         0,
         1744664659000
        ],
        [
         "dbfs:/pyspark/video-streaming-data/module3-transform/optimization/optimized_output/_committed_6586057143507670899",
         "_committed_6586057143507670899",
         35,
         1744499442000
        ],
        [
         "dbfs:/pyspark/video-streaming-data/module3-transform/optimization/optimized_output/device_type=Mobile/",
         "device_type=Mobile/",
         0,
         1744497246000
        ],
        [
         "dbfs:/pyspark/video-streaming-data/module3-transform/optimization/optimized_output/device_type=TV/",
         "device_type=TV/",
         0,
         1744497246000
        ],
        [
         "dbfs:/pyspark/video-streaming-data/module3-transform/optimization/optimized_output/device_type=Tablet/",
         "device_type=Tablet/",
         0,
         1744497247000
        ],
        [
         "dbfs:/pyspark/video-streaming-data/module3-transform/optimization/optimized_output/device_type=Web/",
         "device_type=Web/",
         0,
         1744497246000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Partition by columns that are frequently used in filters\n",
    "\n",
    "# Save to parquet with partitioning\n",
    "print(\"Partitioned Writes Demonstration:\")\n",
    "print(\"--------------------------------\")\n",
    "output_path = \"/pyspark/video-streaming-data/module3-transform/optimization/optimized_output\"\n",
    "\n",
    "# See the impact of partitioning\n",
    "print(\"Writing data partitioned by device_type...\")\n",
    "df.write.partitionBy(\"device_type\").mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"\\nPartition structure created at {output_path}:\")\n",
    "display(dbutils.fs.ls(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a0c51fd-55b9-4f81-9616-d349b57c22ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nBenefits of partitioned writes:\")\n",
    "print(\"1. Enables partition pruning - Spark can skip irrelevant partitions\")\n",
    "print(\"2. Enables parallel reads - Different partitions can be read simultaneously\")\n",
    "print(\"3. Supports partition-aware queries - Filters on partition columns are much faster\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Module 3, Clip 9 - Optimization Techniques for PySpark ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}