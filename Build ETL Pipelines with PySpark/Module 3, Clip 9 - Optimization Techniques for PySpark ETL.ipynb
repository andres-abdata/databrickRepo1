{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46826f27-1a58-4e2d-9229-0c0f63ca30a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optimization Techniques for PySpark ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85fa911a-b8d8-44c3-945e-a425dbf59c60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load our data\n",
    "display(dbutils.fs.ls(\"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/Exercise2/\"))\n",
    "file_path = \"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/Exercise2/\"\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "user_path = \"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/Exercise2/users.csv\"\n",
    "users_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(user_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31aeeeff-1ca4-4d24-9915-29e8c105e990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### OPTIMIZATION 1: BROADCAST JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9aeee4-b2aa-4b00-94d5-2a86f152d8e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Regular join (will use shuffle)\n",
    "# First, let's disable automatic broadcasting to clearly show the difference\n",
    "print(\"Disabling automatic broadcast join optimization...\")\n",
    "original_broadcast_threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")  # Disable automatic broadcasting\n",
    "\n",
    "# Clear everything to ensure a clean demonstration\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# 1. Regular join (will use shuffle)\n",
    "print(\"\\n1. REGULAR JOIN WITH SHUFFLE:\")\n",
    "print(\"-----------------------------\")\n",
    "regular_join = df.join(users_df, on=\"user_id\")\n",
    "print(\"Execution plan for regular join:\")\n",
    "regular_join.explain(mode=\"formatted\")\n",
    "\n",
    "# Look for Exchange hashpartitioning in the plan - this indicates shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6597b88c-8b3f-4677-beeb-f8932d895ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Explicit broadcast join\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "print(\"\\n2. BROADCAST JOIN:\")\n",
    "print(\"-----------------\")\n",
    "broadcast_join = df.join(broadcast(users_df), on=\"user_id\")\n",
    "print(\"Execution plan for broadcast join:\")\n",
    "broadcast_join.explain(mode=\"formatted\")\n",
    "\n",
    "# Look for BroadcastExchange or BroadcastHashJoin in the plan - this indicates broadcasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1be68ea-d500-49c9-829a-1dca7c849604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare query times\n",
    "import time\n",
    "\n",
    "print(\"\\nComparing performance:\")\n",
    "print(\"---------------------\")\n",
    "\n",
    "# Time the regular join\n",
    "start_time = time.time()\n",
    "regular_count = regular_join.count()\n",
    "regular_time = time.time() - start_time\n",
    "print(f\"Regular join time: {regular_time:.2f} seconds for {regular_count} records\")\n",
    "\n",
    "# Time the broadcast join\n",
    "start_time = time.time()\n",
    "broadcast_count = broadcast_join.count()\n",
    "broadcast_time = time.time() - start_time\n",
    "print(f\"Broadcast join time: {broadcast_time:.2f} seconds for {broadcast_count} records\")\n",
    "\n",
    "print(f\"Speedup: {regular_time/broadcast_time:.2f}x faster with broadcast join\")\n",
    "\n",
    "# Restore original broadcast threshold\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", original_broadcast_threshold)\n",
    "print(f\"\\nRestored broadcast threshold to original value: {original_broadcast_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4d3a78a-391b-430d-93a5-77e9bda7b107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### OPTIMIZATION 2: CACHING STRATEGIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc3476fa-5bae-4551-b0fe-027d2b6cdf1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cache intermediate results that are used multiple times\n",
    "import time\n",
    "\n",
    "print(\"Caching demonstration:\")\n",
    "# First, clear any existing cache\n",
    "df.unpersist(blocking=True)\n",
    "\n",
    "# Non-cached approach\n",
    "start_time = time.time()\n",
    "result1 = df.groupBy(\"device_type\").count()\n",
    "result1.show()\n",
    "\n",
    "result2 = df.groupBy(\"device_type\").agg({\"duration_seconds\": \"mean\"})\n",
    "result2.show()\n",
    "end_time = time.time()\n",
    "print(f\"Non-cached execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Cached approach\n",
    "df.cache() # Cache the dataframe\n",
    "df.count() # Force cache evaluation\n",
    "start_time = time.time()\n",
    "result1 = df.groupBy(\"device_type\").count()\n",
    "result1.show()\n",
    "\n",
    "result2 = df.groupBy(\"device_type\").agg({\"duration_seconds\": \"mean\"})\n",
    "result2.show()\n",
    "end_time = time.time()\n",
    "print(f\"Cached execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Remember to unpersist when done\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18c806b8-3229-4150-9135-1bb713b7c90b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### OPTIMIZATION 3: PARTITIONED WRITES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c7af2b-235c-4837-afa9-80bde95da0b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Partition by columns that are frequently used in filters\n",
    "\n",
    "# Save to parquet with partitioning\n",
    "print(\"Partitioned Writes Demonstration:\")\n",
    "print(\"--------------------------------\")\n",
    "output_path = \"/pyspark/video-streaming-data/module3-transform/optimization/optimized_output\"\n",
    "\n",
    "# See the impact of partitioning\n",
    "print(\"Writing data partitioned by device_type...\")\n",
    "df.write.partitionBy(\"device_type\").mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"\\nPartition structure created at {output_path}:\")\n",
    "display(dbutils.fs.ls(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a0c51fd-55b9-4f81-9616-d349b57c22ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nBenefits of partitioned writes:\")\n",
    "print(\"1. Enables partition pruning - Spark can skip irrelevant partitions\")\n",
    "print(\"2. Enables parallel reads - Different partitions can be read simultaneously\")\n",
    "print(\"3. Supports partition-aware queries - Filters on partition columns are much faster\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Module 3, Clip 9 - Optimization Techniques for PySpark ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
