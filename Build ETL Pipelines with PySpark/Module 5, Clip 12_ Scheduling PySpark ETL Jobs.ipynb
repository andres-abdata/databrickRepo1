{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1093bbd0-4438-48fe-b0dd-9fdc87cc80bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Scheduling ETL Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e939014e-d59c-4ae7-8682-5eef77bc299b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c355f3a8-bfc1-429c-96fc-60c11a3b221e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's define our data path\n",
    "base_path = \"/pyspark/video-streaming-data\"\n",
    "events_path = f\"{base_path}/module5-orchestration/scheduling/daily_events\"\n",
    "\n",
    "# First, let's list all the daily event files to see what we're working with\n",
    "event_files = dbutils.fs.ls(events_path)\n",
    "for file_info in event_files:\n",
    "    if file_info.name.endswith('.csv'):\n",
    "        print(f\"Found event file: {file_info.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b5fde0-cbed-495b-9941-0a63a0f92dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's load one of the daily event files to explore the data\n",
    "sample_file_path = f\"{events_path}/events_2023-09-01.csv\"\n",
    "daily_events_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(sample_file_path)\n",
    "\n",
    "print(\"Sample of daily streaming events:\")\n",
    "daily_events_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5288565d-719d-4b98-b336-bca61f16beb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now, let's define a processing function that we would use in our Airflow DAG\n",
    "def process_daily_events(execution_date):\n",
    "    \"\"\"\n",
    "    Process daily video streaming events\n",
    "    This function would be called by Airflow for each execution date\n",
    "    \"\"\"\n",
    "    # Format the date as expected in our filenames\n",
    "    date_str = execution_date.strftime(\"%Y-%m-%d\")\n",
    "    file_path = f\"{events_path}/events_{date_str}.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    try:\n",
    "        # Load the daily events\n",
    "        print(f\"Processing events for {date_str}\")\n",
    "        daily_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "        \n",
    "        # Perform ETL operations\n",
    "        # 1. Clean data\n",
    "        cleaned_df = daily_df.dropDuplicates([\"event_id\"]).na.fill(\"\", [\"error_type\"])\n",
    "        \n",
    "        # 2. Transform - calculate metrics\n",
    "        metrics_df = cleaned_df.groupBy(\"device_type\", \"country\") \\\n",
    "            .agg(\n",
    "                F.count(\"*\").alias(\"total_events\"),\n",
    "                F.avg(\"duration_seconds\").alias(\"avg_duration\"),\n",
    "                F.sum(F.when(F.col(\"error_type\") != \"\", 1).otherwise(0)).alias(\"error_count\")\n",
    "            )\n",
    "        \n",
    "        # 3. Load - save processed data in partitioned format\n",
    "        output_path = f\"{base_path}/module5-orchestration/scheduling/processed_metrics/date={date_str}\"\n",
    "        metrics_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        \n",
    "        print(f\"Successfully processed and saved metrics for {date_str}\")\n",
    "        return {\"date\": date_str, \"record_count\": daily_df.count(), \"status\": \"success\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {date_str}: {str(e)}\")\n",
    "        return {\"date\": date_str, \"status\": \"failed\", \"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99796ea0-9884-45be-8fbb-649ddbf33552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's manually run this function to test it for a sample date\n",
    "test_date = datetime(2023, 9, 1)\n",
    "result = process_daily_events(test_date)\n",
    "print(f\"Processing result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82012196-4265-4534-b729-672a9aa26e84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's check the output data\n",
    "processed_path = f\"{base_path}/module5-orchestration/scheduling/processed_metrics/date=2023-09-01\"\n",
    "try:\n",
    "    processed_df = spark.read.parquet(processed_path)\n",
    "    print(\"Processed metrics data:\")\n",
    "    processed_df.display()\n",
    "except:\n",
    "    print(\"No processed data found - check if processing function executed successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Module 5, Clip 12: Scheduling PySpark ETL Jobs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}