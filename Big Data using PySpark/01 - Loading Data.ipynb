{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4694bb75-0869-4a46-9b9f-75c8fca4db8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "To get set up, do these tasks first: \n",
    "\n",
    "- Upload the customer_data* and transactions_data* files to a cloud bucket or follow the 00 - Generating Data notebook to generate them.\n",
    "- Regardless of how you get the files into your storage, you will have to replace the paths I use in the code below with the paths that make sense for your environment. The ones I use are for accessing Azure Data Lake Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae0663b-3cdf-470f-88a1-2594a378377c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First I set my Azure storage configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e3c255ce-7851-4850-8ccd-f798372e3da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This cell sets all the configuration parameters to connect to Azure Data Lake\n",
    "spark.conf.set(\"fs.azure.account.auth.type.<account_name>.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.<account_name>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.<account_name>.dfs.core.windows.net\", \"****************************\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.<account_name>.dfs.core.windows.net\", \"*******************************\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<account_name>.dfs.core.windows.net\", \"https://login.microsoftonline.com/************************/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05dc8896-caea-4f87-8175-6c80d31c6293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Verify that cloud storage is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950ffc86-3822-465e-8238-ed8860012cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_files = dbutils.fs.ls(\"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/\")\n",
    "filered_files = [file for file in all_files if (file.name.endswith(\".csv\") or file.name.endswith(\".json\") or file.name.endswith(\".orc\") or file.name.endswith(\".parquet\")) and file.name.startswith(\"customers_data\")]\n",
    "display(filered_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad3def8e-498f-4e8a-a66a-7fc6a31f3495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- First, define the paths of the files we are reading.\n",
    "- Replace with what is appropriate with your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca58666-9ed3-4db5-a2f9-252fbe0c7786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define paths for each file format in ADLS\n",
    "csv_path = \"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net//customers_data.csv\"\n",
    "json_path = \"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net//customers_data.json\"\n",
    "orc_path = \"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net//customers_data.orc\"\n",
    "parquet_path = \"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net//customers_data.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ff0cd29-4a85-425b-9adc-5586f821557a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's load and verify the dataframes for each format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04a6709-12ad-43b2-a240-e4aa4c29d104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First, load CVS and use the show command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d98a573-7081-4a1e-8406-c9e80b827bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_csv = (\n",
    "    spark.read\n",
    "        .option(\"header\", \"true\")      # Assume first row as header\n",
    "        .option(\"inferSchema\", \"true\")  # To infer Data types\n",
    "        .csv(csv_path)\n",
    ")\n",
    "print(\"CSV Data\")\n",
    "display(df_csv)\n",
    "\n",
    "# df_csv = (\n",
    "#     spark.read\n",
    "#          .option(\"header\", \"true\")      # Assume first row as header\n",
    "#          .option(\"inferSchema\", \"true\") # Infer data types\n",
    "#          .csv(csv_path)\n",
    "# )\n",
    "# print(\"CSV Data:\")\n",
    "# df_csv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cab4ee7-894b-45fd-9295-f5670d1f81e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The display function is more user friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d9da885-6af6-4c8d-a25d-60fdb94bd5d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"CSV Data:\")\n",
    "df_csv.limit(20).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc85ef58-f89f-4127-bf77-4276c3339547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Load JSON and print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70ba999-b40a-4c0e-84b4-3cd9b87a4f88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json = (\n",
    "    spark.read\n",
    "         .json(json_path)\n",
    ")\n",
    "\n",
    "print(\"json Schema:\")\n",
    "df_json.printSchema()\n",
    "print(\"json Data:\")\n",
    "df_json.limit(20).display()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_json = (\n",
    "#     spark.read\n",
    "#          .json(json_path)\n",
    "# )\n",
    "\n",
    "# print(\"JSON Schema:\")\n",
    "# df_json.printSchema()\n",
    "# print(\"JSON Data:\")\n",
    "# df_json.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13982fe1-8a3c-408f-9e0b-b162afd7fcba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Load ORC and describe the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba174ee4-cc24-4590-9898-b9a09f9a880d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_orc = (\n",
    "    spark.read.orc(orc_path)\n",
    ")\n",
    "\n",
    "df_orc.describe().display()\n",
    "\n",
    "# df_orc = spark.read.orc(orc_path)\n",
    "\n",
    "# print(\"ORC Description:\")\n",
    "# df_orc.describe().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "554af588-f69b-44a5-ab37-9870738d196a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finally, load parquet and check the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac61a05f-9fbc-4d47-a725-2ff6ee542ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(parquet_path)\n",
    "\n",
    "print(\"Parquet Summary:\")\n",
    "df_parquet.summary().display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01 - Loading Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
