{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4694bb75-0869-4a46-9b9f-75c8fca4db8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "To get set up, do these tasks first: \n",
    "\n",
    "- Get service credentials: Client ID `<aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee>` and Client Credential `<NzQzY2QzYTAtM2I3Zi00NzFmLWI3MGMtMzc4MzRjZmk=>`. Follow the instructions in [Create service principal with portal](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal). \n",
    "- Get directory ID `<ffffffff-gggg-hhhh-iiii-jjjjjjjjjjjj>`: This is also referred to as *tenant ID*. Follow the instructions in [Get tenant ID](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal#get-tenant-id). \n",
    "- If you haven't set up the service app, follow this [tutorial](https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-extract-load-sql-data-warehouse). Set access at the root directory or desired folder level to the service or everyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c255ce-7851-4850-8ccd-f798372e3da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This cell sets all the configuration parameters to connect to Azure Data Lake\n",
    "spark.conf.set(\"fs.azure.account.auth.type.<account_name>.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.<account_name>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.<account_name>.dfs.core.windows.net\", \"****************************\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.<account_name>.dfs.core.windows.net\", \"*******************************\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<account_name>.dfs.core.windows.net\", \"https://login.microsoftonline.com/************************/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05dc8896-caea-4f87-8175-6c80d31c6293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Verify that cloud storage is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950ffc86-3822-465e-8238-ed8860012cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='abfss://pyspark@warnerdatalake.dfs.core.windows.net/exports/', name='exports/', size=0, modificationTime=1740581924000),\n",
       " FileInfo(path='abfss://pyspark@warnerdatalake.dfs.core.windows.net/imports/', name='imports/', size=0, modificationTime=1740581918000)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"abfss://pyspark@warnerdatalake.dfs.core.windows.net/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03146a39-2722-498c-9844-e9ab54496c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a Customers dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe225ee-339f-4517-a0be-7d7a32785fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------------------+---+-------+\n|customer_id|first_name|last_name|               email|age|country|\n+-----------+----------+---------+--------------------+---+-------+\n|          1|   First_1|   Last_1|First_1.Last_1@ex...| 40| Canada|\n|          2|   First_2|   Last_2|First_2.Last_2@ex...| 55|    USA|\n|          3|   First_3|   Last_3|First_3.Last_3@ex...| 59|    USA|\n|          4|   First_4|   Last_4|First_4.Last_4@ex...| 49| Canada|\n|          5|   First_5|   Last_5|First_5.Last_5@ex...| 58| Canada|\n|          6|   First_6|   Last_6|First_6.Last_6@ex...| 55|    USA|\n|          7|   First_7|   Last_7|First_7.Last_7@ex...| 32|    USA|\n|          8|   First_8|   Last_8|First_8.Last_8@ex...| 56|    USA|\n|          9|   First_9|   Last_9|First_9.Last_9@ex...| 47|    USA|\n|         10|  First_10|  Last_10|First_10.Last_10@...| 30| Canada|\n+-----------+----------+---------+--------------------+---+-------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Number of customers to simulate\n",
    "num_customers = 10000\n",
    "\n",
    "# List of countries and corresponding skewed weights\n",
    "countries = [\"USA\", \"Canada\", \"UK\"]\n",
    "country_weights = [0.7, 0.2, 0.1]  # 70% USA, 20% Canada, 10% UK\n",
    "\n",
    "# Function to get a weighted random selection\n",
    "def get_weighted_country():\n",
    "    return F.when(F.rand() < country_weights[0], countries[0]) \\\n",
    "            .when(F.rand() < (country_weights[0] + country_weights[1]), countries[1]) \\\n",
    "            .otherwise(countries[2])\n",
    "\n",
    "# Create a customer dataframe with additional columns: age and country\n",
    "df_customers = (\n",
    "    spark.range(1, num_customers + 1)\n",
    "         .withColumnRenamed(\"id\", \"customer_id\")\n",
    "         .withColumn(\"first_name\", F.concat(F.lit(\"First_\"), F.col(\"customer_id\")))\n",
    "         .withColumn(\"last_name\", F.concat(F.lit(\"Last_\"), F.col(\"customer_id\")))\n",
    "         .withColumn(\"email\", F.concat(F.col(\"first_name\"), F.lit(\".\"), F.col(\"last_name\"), F.lit(\"@example.com\")))\n",
    "         # Add skewed age column: 70% above 40, 30% below 40\n",
    "         .withColumn(\"age\", \n",
    "                     F.when(F.rand() < 0.3, (F.floor(F.rand() * 22) + 18))  # 18 to 39\n",
    "                      .otherwise((F.floor(F.rand() * 21) + 40)))           # 40 to 60\n",
    "         # Add skewed country column using weighted selection\n",
    "         .withColumn(\"country\", get_weighted_country())\n",
    ")\n",
    "\n",
    "df_customers.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c01b0c2c-7edb-4d76-97c0-ec8e4c796909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Save it into storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a16e86-c66d-4bde-9e7b-0a4e06c2119b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_output_path = \"abfss://pyspark@warnerdatalake.dfs.core.windows.net//imports//customers_data\"\n",
    "df_customers.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7fe92f3-aaa4-4a4a-a4e1-ab36ca839ef4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orc_output_path = \"abfss://pyspark@warnerdatalake.dfs.core.windows.net//imports//customers_data\"\n",
    "df_customers.coalesce(1).write.mode(\"overwrite\").orc(orc_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b0acddf-44a2-433c-8319-daf62e8c264b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pqt_output_path = \"abfss://pyspark@warnerdatalake.dfs.core.windows.net//imports//customers_data\"\n",
    "df_customers.coalesce(1).write.mode(\"overwrite\").parquet(pqt_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba0a4d1-8e23-4897-be37-0250906312b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_output_path = \"abfss://pyspark@warnerdatalake.dfs.core.windows.net//imports//customers_data\"\n",
    "df_customers.coalesce(1).write.mode(\"overwrite\").json(json_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e94a403-eee4-48d8-b608-35910d013560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After the files are generated, you would need to rename them to the \"customers_data*.\" pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eafe7a3c-10c2-46f5-86af-b91fe5a0324f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Generate the transactions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f364411f-6b22-409a-a280-958c6d086b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------------+------+-----------+\n|transaction_id|customer_id|transaction_date|amount|   category|\n+--------------+-----------+----------------+------+-----------+\n|             1|       3065|      2025-03-17| 76.10|    Clothes|\n|             2|       3274|      2025-02-18| 91.91|    Clothes|\n|             3|        130|      2025-01-10| 11.81|Accessories|\n|             4|        320|      2025-03-06| 20.37|  Furniture|\n|             5|       6480|      2025-03-22| 12.31|     Beauty|\n+--------------+-----------+----------------+------+-----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "# Define the list of categories\n",
    "categories = [\"Clothes\", \"Gadgets\", \"Food\", \"Toys\", \"Books\", \"Furniture\", \"Electronics\", \"Sports\", \"Beauty\", \"Accessories\"]\n",
    "\n",
    "# Create an array column literal with the categories\n",
    "categories_array = F.array(*[F.lit(cat) for cat in categories])\n",
    "\n",
    "# Number of transactions to simulate (at least 1M)\n",
    "num_transactions = 1000000\n",
    "\n",
    "df_transactions = (\n",
    "    spark.range(1, num_transactions + 1)\n",
    "         .withColumnRenamed(\"id\", \"transaction_id\")\n",
    "         # Assign a random customer_id from 1 to num_customers (assuming num_customers is defined)\n",
    "         .withColumn(\"customer_id\", (F.rand() * 10000).cast(\"integer\") + 1)\n",
    "         # Generate a random transaction date between 2025-01-01 and 2025-01-31\n",
    "         .withColumn(\"transaction_date\", \n",
    "                     F.expr(\"date_add('2025-01-01', cast(rand() * 90 as int))\"))\n",
    "         # Generate a random transaction amount between 0 and 100, formatted as decimal(10,2)\n",
    "         .withColumn(\"amount\", \n",
    "                     (F.rand() * 100).cast(DecimalType(10,2)))\n",
    "         # Add a category column by randomly selecting one from the categories array\n",
    "         .withColumn(\"category\", \n",
    "                     F.element_at(categories_array, (F.floor(F.rand() * len(categories)) + 1).cast(\"integer\")))\n",
    ")\n",
    "\n",
    "df_transactions.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e81fec09-18d6-4e03-b609-c206a1b431f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pqt_output_path = \"abfss://pyspark@warnerdatalake.dfs.core.windows.net//imports//transactions_data\"\n",
    "df_transactions.coalesce(1).write.mode(\"overwrite\").parquet(pqt_output_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00 - Generating Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}