{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4694bb75-0869-4a46-9b9f-75c8fca4db8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "Make sure you have the files available from previous demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e3c255ce-7851-4850-8ccd-f798372e3da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
     "# This cell sets all the configuration parameters to connect to Azure Data Lake\n",
    "spark.conf.set(\"fs.azure.account.auth.type.<account_name>.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.<account_name>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.<account_name>.dfs.core.windows.net\", \"****************************\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.<account_name>.dfs.core.windows.net\", \"*******************************\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<account_name>.dfs.core.windows.net\", \"https://login.microsoftonline.com/************************/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05dc8896-caea-4f87-8175-6c80d31c6293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Verify that cloud storage is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950ffc86-3822-465e-8238-ed8860012cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"abfss://pyspark@warnerdatalake.dfs.core.windows.net/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "326d0615-e7a0-4d58-a741-bf812deeded8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's load the transactions data with some optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2fe845c-20c6-41b4-b5b0-9518a9e1c06f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>total_amount</th></tr></thead><tbody><tr><td>Food</td><td>3768630.66</td></tr><tr><td>Sports</td><td>3746511.53</td></tr><tr><td>Electronics</td><td>3773395.39</td></tr><tr><td>Books</td><td>3739182.06</td></tr><tr><td>Furniture</td><td>3745521.08</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Food",
         "3768630.66"
        ],
        [
         "Sports",
         "3746511.53"
        ],
        [
         "Electronics",
         "3773395.39"
        ],
        [
         "Books",
         "3739182.06"
        ],
        [
         "Furniture",
         "3745521.08"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"decimal(20,2)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the path to the transactions dataset\n",
    "parquet_path = \"abfss://pyspark@warnerdatalake.dfs.core.windows.net//imports//transactions_data.parquet\"\n",
    "\n",
    "# Apply column pruning and predicate pushdown\n",
    "df_optimized = (\n",
    "    spark.read.parquet(parquet_path)\n",
    "        .select(\"category\", \"amount\")  # Column pruning\n",
    "        .filter(F.col(\"amount\") > 50)  # Predicate pushdown\n",
    "        .groupBy(\"category\")\n",
    "        .agg(F.sum(\"amount\").alias(\"total_amount\"))\n",
    ")\n",
    "\n",
    "# Display the optimized DataFrame\n",
    "df_optimized.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95174b70-6b20-40fa-b75d-bfaa2c9ec66d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Verify the predicate pushdown in the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b205b1-e630-4e93-89c3-dbbf3e08df8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Aggregate ['category], ['category, 'sum('amount) AS total_amount#60]\n+- Filter (amount#48 > cast(cast(50 as decimal(2,0)) as decimal(10,2)))\n   +- Project [category#49, amount#48]\n      +- Relation [transaction_id#45L,customer_id#46,transaction_date#47,amount#48,category#49] parquet\n\n== Analyzed Logical Plan ==\ncategory: string, total_amount: decimal(20,2)\nAggregate [category#49], [category#49, sum(amount#48) AS total_amount#60]\n+- Filter (amount#48 > cast(cast(50 as decimal(2,0)) as decimal(10,2)))\n   +- Project [category#49, amount#48]\n      +- Relation [transaction_id#45L,customer_id#46,transaction_date#47,amount#48,category#49] parquet\n\n== Optimized Logical Plan ==\nAggregate [category#49], [category#49, sum(amount#48) AS total_amount#60]\n+- Project [category#49, amount#48]\n   +- Filter (isnotnull(amount#48) AND (amount#48 > 50.00))\n      +- Relation [transaction_id#45L,customer_id#46,transaction_date#47,amount#48,category#49] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   HashAggregate(keys=[category#49], functions=[finalmerge_sum(merge sum#68, isEmpty#69) AS sum(amount#48)#61], output=[category#49, total_amount#60])\n   +- Exchange hashpartitioning(category#49, 200), ENSURE_REQUIREMENTS, [plan_id=169]\n      +- HashAggregate(keys=[category#49], functions=[partial_sum(amount#48) AS (sum#68, isEmpty#69)], output=[category#49, sum#68, isEmpty#69])\n         +- Filter (isnotnull(amount#48) AND (amount#48 > 50.00))\n            +- FileScan parquet [amount#48,category#49] Batched: true, DataFilters: [isnotnull(amount#48), (amount#48 > 50.00)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[abfss://pyspark@warnerdatalake.dfs.core.windows.net/imports/transactio..., PartitionFilters: [], PushedFilters: [IsNotNull(amount), GreaterThan(amount,50.00)], ReadSchema: struct<amount:decimal(10,2),category:string>\n\n"
     ]
    }
   ],
   "source": [
    "#Look for PushedFilters: [GreaterThan(amount,50)] in the output, which confirms predicate pushdown is happening.\n",
    "df_optimized.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d9446f6-39b3-4d93-9ea6-3e5fc99d4a70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can optimize retrieval and fit a query pattern with partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85bb768c-c8bd-4085-8aba-1b32d0c8b822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First write the dataframe as a partitioned set of folders\n",
    "partitioned_path = \"abfss://pyspark@warnerdatalake.dfs.core.windows.net//exports//transactions_partitioned\"\n",
    "df_transactions = spark.read.parquet(parquet_path)\n",
    "df_transactions.write.mode(\"overwrite\").partitionBy(\"category\", \"transaction_date\").parquet(partitioned_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fbb8752-7267-4ed9-991f-169aa7d18fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Then we can read it back with the right filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c40f4a7-8edb-4d35-9614-6cc32d039c53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>transaction_id</th><th>customer_id</th><th>amount</th><th>category</th><th>transaction_date</th></tr></thead><tbody><tr><td>139</td><td>9976</td><td>19.82</td><td>Electronics</td><td>2025-01-03</td></tr><tr><td>1445</td><td>1895</td><td>9.69</td><td>Electronics</td><td>2025-01-03</td></tr><tr><td>2828</td><td>8191</td><td>79.55</td><td>Electronics</td><td>2025-01-03</td></tr><tr><td>4393</td><td>236</td><td>94.39</td><td>Electronics</td><td>2025-01-03</td></tr><tr><td>4470</td><td>5998</td><td>79.38</td><td>Electronics</td><td>2025-01-03</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         139,
         9976,
         "19.82",
         "Electronics",
         "2025-01-03"
        ],
        [
         1445,
         1895,
         "9.69",
         "Electronics",
         "2025-01-03"
        ],
        [
         2828,
         8191,
         "79.55",
         "Electronics",
         "2025-01-03"
        ],
        [
         4393,
         236,
         "94.39",
         "Electronics",
         "2025-01-03"
        ],
        [
         4470,
         5998,
         "79.38",
         "Electronics",
         "2025-01-03"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "transaction_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "transaction_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_partitioned = (\n",
    "    spark.read.parquet(partitioned_path)\n",
    "        .filter(F.col(\"category\") == \"Electronics\")  # Partition pruning\n",
    ")\n",
    "\n",
    "df_partitioned.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "068fec6b-9c6b-4f8e-8fde-2ced1efd03b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Verify the partition pruning in the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584b1350-dd16-4e71-8f6c-a7964fea953b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Filter '`=`('category, Electronics)\n+- Relation [transaction_id#103L,customer_id#104,amount#105,category#106,transaction_date#107] parquet\n\n== Analyzed Logical Plan ==\ntransaction_id: bigint, customer_id: int, amount: decimal(10,2), category: string, transaction_date: date\nFilter (category#106 = Electronics)\n+- Relation [transaction_id#103L,customer_id#104,amount#105,category#106,transaction_date#107] parquet\n\n== Optimized Logical Plan ==\nFilter (isnotnull(category#106) AND (category#106 = Electronics))\n+- Relation [transaction_id#103L,customer_id#104,amount#105,category#106,transaction_date#107] parquet\n\n== Physical Plan ==\n*(1) ColumnarToRow\n+- FileScan parquet [transaction_id#103L,customer_id#104,amount#105,category#106,transaction_date#107] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[abfss://pyspark@warnerdatalake.dfs.core.windows.net/exports/transactio..., PartitionFilters: [isnotnull(category#106), (category#106 = Electronics)], PushedFilters: [], ReadSchema: struct<transaction_id:bigint,customer_id:int,amount:decimal(10,2)>\n\n"
     ]
    }
   ],
   "source": [
    "# Look for PartitionFilters: [isnotnull(category), (category = Electronics)], which confirms partition pruning.\n",
    "df_partitioned.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d74b8c6-c519-4430-8bfc-50f701858524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Some file formats have even more optimizations like delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "915174f4-02c5-4e0a-a78f-acbd9bd862e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_path = \"abfss://pyspark@warnerdatalake.dfs.core.windows.net//exports//transactions_delta\"\n",
    "\n",
    "df_transactions.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"category\", \"transaction_date\").save(delta_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d6b7404-2fe5-408f-8754-e61eb0163183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delta supports Z-Ordering, which improves range-based queries (e.g., amount > X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c671f868-ea08-40ca-9824-41f202fe0513",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,isFull:boolean,approxClusteringQuality:double,approxClusteringCoverage:double,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numIdealFilesWithTrimmedStringMaxValue:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Optimize Delta Table Storage\n",
    "delta_table.optimize().executeZOrderBy(\"amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89d2f141-b5e4-49d5-979c-c8e95d245486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To confirm all optimizations, let's run .explain() on the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5984678a-e9ae-428d-a7cd-b8953a4c7423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Filter 'and('`>`('amount, 50), '`=`('category, Electronics))\n+- Relation [transaction_id#774L,customer_id#775,transaction_date#776,amount#777,category#778] parquet\n\n== Analyzed Logical Plan ==\ntransaction_id: bigint, customer_id: int, transaction_date: date, amount: decimal(10,2), category: string\nFilter ((amount#777 > cast(cast(50 as decimal(2,0)) as decimal(10,2))) AND (category#778 = Electronics))\n+- Relation [transaction_id#774L,customer_id#775,transaction_date#776,amount#777,category#778] parquet\n\n== Optimized Logical Plan ==\nFilter ((isnotnull(amount#777) AND isnotnull(category#778)) AND ((amount#777 > 50.00) AND (category#778 = Electronics)))\n+- Relation [transaction_id#774L,customer_id#775,transaction_date#776,amount#777,category#778] parquet\n\n== Physical Plan ==\n*(1) Project [transaction_id#774L, customer_id#775, transaction_date#776, amount#777, category#778]\n+- *(1) Filter ((if (isnotnull(_databricks_internal_edge_computed_column_skip_row#928)) (_databricks_internal_edge_computed_column_skip_row#928 = false) else isnotnull(raise_error(DELTA_SKIP_ROW_COLUMN_NOT_FILLED, map(keys: [], values: []), NullType)) AND isnotnull(amount#777)) AND (amount#777 > 50.00))\n   +- *(1) ColumnarToRow\n      +- FileScan parquet [transaction_id#774L,customer_id#775,amount#777,_databricks_internal_edge_computed_column_skip_row#928,category#778,transaction_date#776] Batched: true, DataFilters: [isnotnull(amount#777), (amount#777 > 50.00)], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[abfss://pyspark@warnerdatalake.dfs.core.windows.net/exports/trans..., PartitionFilters: [isnotnull(category#778), (category#778 = Electronics)], PushedFilters: [IsNotNull(amount), GreaterThan(amount,50.00)], ReadSchema: struct<transaction_id:bigint,customer_id:int,amount:decimal(10,2),_databricks_internal_edge_compu...\n\n"
     ]
    }
   ],
   "source": [
    "df_delta = (\n",
    "    spark.read.format(\"delta\").load(delta_path)\n",
    "        .filter((F.col(\"amount\") > 50) & (F.col(\"category\") == \"Electronics\"))\n",
    ")\n",
    "\n",
    "df_delta.explain(True)\n",
    "\n",
    "# Expected Execution Plan Output\n",
    "# PushedFilters: [GreaterThan(amount,50)] → ✅ Predicate Pushdown confirmed.\n",
    "# PartitionFilters: [category=Electronics] → ✅ Partition Pruning confirmed.\n",
    "# Z-Ordering by amount → ✅ File skipping optimization confirmed.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "06 - Optimizing Data Movement",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}