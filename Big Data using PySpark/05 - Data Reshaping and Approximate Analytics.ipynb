{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4694bb75-0869-4a46-9b9f-75c8fca4db8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "Make sure you have the files available from previous demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e3c255ce-7851-4850-8ccd-f798372e3da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This cell sets all the configuration parameters to connect to Azure Data Lake\n",
    "spark.conf.set(\"fs.azure.account.auth.type.<account_name>.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.<account_name>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.<account_name>.dfs.core.windows.net\", \"****************************\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.<account_name>.dfs.core.windows.net\", \"*******************************\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<account_name>.dfs.core.windows.net\", \"https://login.microsoftonline.com/************************/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05dc8896-caea-4f87-8175-6c80d31c6293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Verify that cloud storage is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950ffc86-3822-465e-8238-ed8860012cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccc7dda3-b39f-4e0e-bdd7-d8a5c88d5049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's load the transactions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a31e9f00-638a-4b72-9b22-a2642141d2e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Load transactions data from Azure Data Lake\n",
    "parquet_path = \"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/transactions_data.parquet\"\n",
    "\n",
    "df_transactions = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Display sample data\n",
    "df_transactions.limit(100).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47af379a-3547-4e05-84c2-2b60b65b0e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's try some pivoting to reshape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a93ea44-f66d-4687-a77f-0affd4428ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pivot = (\n",
    "    df_transactions.groupBy('customer_id')\n",
    "        .pivot('category')\n",
    "        .agg(F.sum('amount').alias('total_spent'))\n",
    ")\n",
    "\n",
    "df_pivot.limit(100).display()\n",
    "\n",
    "\n",
    "# # Pivot categories into columns\n",
    "\n",
    "# df_pivot = (\n",
    "#     df_transactions\n",
    "#         .groupBy(\"customer_id\")\n",
    "#         .pivot(\"category\")\n",
    "#         .agg(F.sum(\"amount\").alias(\"total_spent\"))\n",
    "# )\n",
    "\n",
    "# df_pivot.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0b6887a-6b95-4cb8-a4d6-43cce5a14e9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can unpivot as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98b3a7d9-0dd9-4836-b2eb-4b21ae11f533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#To unpivit\n",
    " \n",
    "unpivot_columns = df_pivot.columns[1:]\n",
    "df_unpivot = df_pivot.selectExpr(\n",
    "    \"customer_id\",\n",
    "    \"stack(\" + str(len(unpivot_columns)) + \", \" +\n",
    "    \", \".join([f\"'{col}', {col}\" for col in unpivot_columns]) +\n",
    "    \") as (category, total_spent)\"\n",
    ")\n",
    "\n",
    "df_unpivot.limit(100).display()\n",
    "\n",
    "\n",
    "# # Reshaping data back into rows\n",
    "# unpivot_columns = df_pivot.columns[1:]  # Exclude 'customer_id'\n",
    "\n",
    "# df_unpivot = df_pivot.selectExpr(\n",
    "#     \"customer_id\",\n",
    "#     \"stack(\" + str(len(unpivot_columns)) + \", \" +\n",
    "#     \", \".join([f\"'{col}', {col}\" for col in unpivot_columns]) +\n",
    "#     \") as (category, total_spent)\"\n",
    "# )\n",
    "\n",
    "# df_unpivot.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa92081d-5c85-475e-a079-f6f38d78083b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can do approximations on large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b75245-2396-4277-8948-6b635604901a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_grouped = (\n",
    "    df_transactions.groupBy('category')\n",
    "        .agg(F.countDistinct('customer_id').alias('total_amount'))        \n",
    ")\n",
    "df_grouped.limit(100).display()\n",
    "\n",
    "df_grouped_aprox = (\n",
    "    df_transactions.groupBy('category')\n",
    "        .agg(F.approxCountDistinct('customer_id').alias('aprox_total_amount'))\n",
    ")\n",
    "df_grouped_aprox.limit(100).display()\n",
    "\n",
    "# # HyperLogLog for Approximate Distinct Counting\n",
    "\n",
    "# df_hyperloglog = (\n",
    "#     df_transactions\n",
    "#         .groupBy(\"category\")\n",
    "#         .agg(F.approx_count_distinct(\"customer_id\").alias(\"approx_unique_customers\"))\n",
    "# )\n",
    "\n",
    "# df_hyperloglog.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d87618-be30-441b-b795-e134610f9072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bloom filters for membership testing\n",
    "# Define the output path with Bloom Filters enabled\n",
    "bloom_parquet_path = \"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net//exports//transactions_bloom\"\n",
    "\n",
    "# Step 1: Specify the Bloom filter options\n",
    "bloom_filter_options = {\n",
    "    \"spark.sql.parquet.bloom.filter.enabled\": \"true\",  # Enable Bloom filter\n",
    "    \"spark.sql.parquet.bloom.filter.column\": \"customer_id\",  # Apply Bloom filter to customer_id\n",
    "    \"spark.sql.parquet.bloom.filter.expected.ndv\": \"10000\",  # Expected unique customer IDs\n",
    "    \"spark.sql.parquet.bloom.filter.fpp\": \"0.01\"  # 1% false positive probability\n",
    "}\n",
    "\n",
    "# Step 2: Write DataFrame to Parquet with Bloom filter enabled\n",
    "df_transactions.write \\\n",
    "    .option(\"parquet.bloom.filter.enabled\", bloom_filter_options[\"spark.sql.parquet.bloom.filter.enabled\"]) \\\n",
    "    .option(\"parquet.bloom.filter.column\", bloom_filter_options[\"spark.sql.parquet.bloom.filter.column\"]) \\\n",
    "    .option(\"parquet.bloom.filter.expected.ndv\", bloom_filter_options[\"spark.sql.parquet.bloom.filter.expected.ndv\"]) \\\n",
    "    .option(\"parquet.bloom.filter.fpp\", bloom_filter_options[\"spark.sql.parquet.bloom.filter.fpp\"]) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(bloom_parquet_path)\n",
    "\n",
    "print(\"âœ… Transactions data written to Parquet with Bloom Filter enabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231de672-4e5d-4124-8cd6-afa2eaafa552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bloom = spark.read \\\n",
    "    .option(\"parquet.filter.bloom.enabled\", \"true\") \\\n",
    "    .parquet(bloom_parquet_path)\n",
    "\n",
    "# Apply filter on customer_id to check performance\n",
    "df_filtered = df_bloom.filter(F.col(\"customer_id\") == 12345)\n",
    "\n",
    "df_filtered.limit(5).display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05 - Data Reshaping and Approximate Analytics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
