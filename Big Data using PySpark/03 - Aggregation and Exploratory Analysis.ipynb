{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4694bb75-0869-4a46-9b9f-75c8fca4db8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "Make sure you have the files available from previous demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e3c255ce-7851-4850-8ccd-f798372e3da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This cell sets all the configuration parameters to connect to Azure Data Lake\n",
    "spark.conf.set(\"fs.azure.account.auth.type.<account_name>.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.<account_name>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.<account_name>.dfs.core.windows.net\", \"****************************\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.<account_name>.dfs.core.windows.net\", \"*******************************\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<account_name>.dfs.core.windows.net\", \"https://login.microsoftonline.com/************************/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05dc8896-caea-4f87-8175-6c80d31c6293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Verify that cloud storage is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950ffc86-3822-465e-8238-ed8860012cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e28b9968-1656-42bc-9314-2e1c813aa7dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's load the transactions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5fb2f2a-09ae-4562-83b3-6ed95ec87415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Path to the transactions data\n",
    "parquet_path = \"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/transactions_data.parquet\"\n",
    "\n",
    "# Load the transactions data\n",
    "df_transactions = spark.read.parquet(parquet_path)\n",
    "df_transactions.printSchema()\n",
    "\n",
    "# Display the first 5 records\n",
    "df_transactions.limit(5).display()\n",
    "print(df_transactions.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7572716d-f40f-4aeb-9da5-784efcd56766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's do some grouping and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff964d3b-a049-4fd0-8ad5-4dab4b38351b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_category_total = (\n",
    "    df_transactions.groupBy(F.col('category'))\n",
    "    .agg(F.sum('amount').alias('total_amount'))\n",
    "    .orderBy(F.desc('total_amount'))\n",
    ")\n",
    "\n",
    "display(df_category_total)\n",
    "\n",
    "\n",
    "# from pyspark.sql import functions as F\n",
    "# # Total amount per category\n",
    "# df_category_total = (\n",
    "#     df_transactions\n",
    "#         .groupBy(\"category\")\n",
    "#         .agg(F.sum(\"amount\").alias(\"total_amount\"))\n",
    "#         .orderBy(F.col(\"total_amount\").desc())\n",
    "# )\n",
    "\n",
    "# df_category_total.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67580edc-5957-421c-82d2-9f61e925d956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_category_stats = (\n",
    "    df_transactions.groupBy(F.col('category'))\n",
    "    .agg(F.avg('amount').alias('avg_amount'),\n",
    "        F.max('amount').alias('max_amount'),\n",
    "        F.min('amount').alias('min_amount')\n",
    "    )\n",
    "    .orderBy(F.desc('avg_amount'))\n",
    ")\n",
    "\n",
    "df_category_stats.display()\n",
    "\n",
    "\n",
    "# # Average, max and min amount per category ordered by average\n",
    "# df_category_stats = (\n",
    "#     df_transactions\n",
    "#         .groupBy(\"category\")\n",
    "#         .agg(\n",
    "#             F.avg(\"amount\").alias(\"avg_amount\"),\n",
    "#             F.max(\"amount\").alias(\"max_amount\"),\n",
    "#             F.min(\"amount\").alias(\"min_amount\")\n",
    "#         )\n",
    "#         .orderBy(F.col(\"avg_amount\").desc())\n",
    "# )\n",
    "\n",
    "# df_category_stats.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b56e7c68-6572-4669-8924-568f02d8f84b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's do some Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6710a821-4600-4bb7-91a4-95c9ba1bac6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_transactions.select('amount').summary().display()\n",
    "\n",
    "# # Get the statistics for a single column\n",
    "# df_transactions.select(\"amount\").summary().display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d6b0c50-6787-44dc-af55-3e85af8e5b2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_category_distributiion = (\n",
    "    df_transactions.groupBy(F.col('category'))\n",
    "    .agg(F.count('category').alias('count_of_categories'))\n",
    "    .orderBy(F.desc('count_of_categories'))\n",
    ")\n",
    "\n",
    "df_category_distributiion.display()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Calculate the distribution of categories\n",
    "# df_category_distribution = (\n",
    "#     df_transactions\n",
    "#         .groupBy(\"category\")\n",
    "#         .count()\n",
    "#         .orderBy(F.col(\"count\").desc())\n",
    "# )\n",
    "\n",
    "# df_category_distribution.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "256e5061-7bdb-4a00-955b-39ee1d452895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mix aggregations in a single result, top categories by total and average amount\n",
    "\n",
    "df_top_categories = (\n",
    "    df_transactions\n",
    "        .groupBy(\"category\")\n",
    "        .agg(\n",
    "            F.sum(\"amount\").alias(\"total_amount\"),\n",
    "            F.avg(\"amount\").alias(\"avg_amount\"),\n",
    "            F.count(\"*\").alias(\"transaction_count\")\n",
    "        )\n",
    "        .orderBy(F.col(\"total_amount\").desc(), F.col(\"avg_amount\").desc())\n",
    ")\n",
    "\n",
    "df_top_categories.limit(5).display()\n",
    "\n",
    "df_customers = spark.read.parquet(\"abfss://etl1@dbstoragebbpbs73u57xmm.dfs.core.windows.net/customers_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92064ef9-2082-4a11-9c8b-453ba44af0d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_customers.limit(50).display()\n",
    "df_transactions.limit(50).display()\n",
    "\n",
    "\n",
    "# # Calculate the average transaction amount by age group\n",
    "\n",
    "# # Load the customers data\n",
    "# customers_path = \"abfss://pyspark@warnerdatalake.dfs.core.windows.net//imports//customers_data.parquet\"\n",
    "# df_customers = spark.read.parquet(customers_path)\n",
    "\n",
    "# # Join transactions with customers\n",
    "# df_joined = df_transactions.join(df_customers, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "# # Add age group column and calculate average spending by age group\n",
    "# df_age_group_spending = (\n",
    "#     df_joined\n",
    "#         .withColumn(\n",
    "#             \"age_group\", \n",
    "#             F.when(F.col(\"age\") < 30, \"Under 30\")\n",
    "#              .when((F.col(\"age\") >= 30) & (F.col(\"age\") < 50), \"30-49\")\n",
    "#              .otherwise(\"50 and Above\")\n",
    "#         )\n",
    "#         .groupBy(\"age_group\")\n",
    "#         .agg(F.avg(\"amount\").alias(\"avg_spending\"))\n",
    "#         .orderBy(F.col(\"avg_spending\").desc())\n",
    "# )\n",
    "\n",
    "# df_age_group_spending.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67a41bc9-ebef-4c8a-87df-9ad12a0fbb66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Join Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be20a159-91c1-49af-be6a-ba68e490bfb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([Row(name='Alice',age=2),Row(name='Bob',age=5)])\n",
    "df2 = spark.createDataFrame([Row(name='Tom',height=80),Row(name='Bob',height=85)])\n",
    "df3 = spark.createDataFrame([\n",
    "  Row(name='Alice',age=10,height=80),\n",
    "  Row(name='Bob',age=5,height=None),\n",
    "  Row(name='Tome',age=None,height=None),\n",
    "  Row(name=None,age=None,height=None)\n",
    "  ])\n",
    "\n",
    "df.display()\n",
    "df2.display()\n",
    "df3.display()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "047ccba7-2bbb-48bf-8f62-0efe8d255509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.join(df2,df.name==df2.name).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6317d65-190c-464a-9319-10e6b3891b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.join(df3,(df.name == df3.name) & (df.age == df3.age),'inner')\\\n",
    "    .select(df.name,df.age,df3.height).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b916a5-e798-4de1-9e34-fd556cd21982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.alias('a').join(df2.alias('b'),F.col('a.name')==F.col('b.name'),'outer').orderBy(F.desc(df.name)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c4c92c-a04e-42f7-a21c-8cc3724d9d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.alias('a').join(df2.alias('b'),F.col('a.name')==F.col('b.name'),'left_outer').orderBy(F.desc(df.name)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ffdb469-3901-4c99-bba2-b66ccc19c565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.alias('a').join(df2.alias('b'),F.col('a.name')==F.col('b.name'),'right_outer').orderBy(F.desc(df.name))\\\n",
    "    .display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03 - Aggregation and Exploratory Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
